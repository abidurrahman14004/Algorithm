{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/abidur14004/illinosis-doc?scriptVersionId=269050689\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","execution_count":1,"id":"cd5b25e8","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2025-10-19T02:57:00.389177Z","iopub.status.busy":"2025-10-19T02:57:00.388935Z","iopub.status.idle":"2025-10-19T02:57:44.830641Z","shell.execute_reply":"2025-10-19T02:57:44.829911Z"},"papermill":{"duration":44.449963,"end_time":"2025-10-19T02:57:44.832119","exception":false,"start_time":"2025-10-19T02:57:00.382156","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","======================================================================\n","  LEAKAGE-FREE ILLINOIS DOC PREPROCESSING\n","======================================================================\n","\n","======================================================================\n","  ILLINOIS DOC DATASET - LEAKAGE-FREE PREPROCESSING PIPELINE\n","======================================================================\n","\n","[STEP 1] Loading data and engineering basic features...\n","=== Starting Basic Feature Engineering ===\n","Initial shape: (61110, 22)\n","[1/3] Engineering demographic features...\n","[2/3] Cleaning categorical features...\n","[3/3] Engineering image features...\n","\n","Removed 395 rows with invalid BMI\n","Final shape after basic feature engineering: (60715, 28)\n","\n","=== Basic Feature Engineering Summary ===\n","Total features created: 28\n","\n","[STEP 2] Matching images to data...\n","\n","Matched 60715/60715 records to images (100.0%)\n","\n","[STEP 3] Validating images...\n","Fast validating 60715 images...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3bfa4c05bb0d43009a72d93bfc772d44","version_major":2,"version_minor":0},"text/plain":["Quick validation:   0%|          | 0/60715 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\n","✗ Error during preprocessing: 1288\n"]},{"name":"stderr","output_type":"stream","text":["Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\", line 3805, in get_loc\n","    return self._engine.get_loc(casted_key)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"index.pyx\", line 167, in pandas._libs.index.IndexEngine.get_loc\n","  File \"index.pyx\", line 196, in pandas._libs.index.IndexEngine.get_loc\n","  File \"pandas/_libs/hashtable_class_helper.pxi\", line 2606, in pandas._libs.hashtable.Int64HashTable.get_item\n","  File \"pandas/_libs/hashtable_class_helper.pxi\", line 2630, in pandas._libs.hashtable.Int64HashTable.get_item\n","KeyError: 1288\n","\n","The above exception was the direct cause of the following exception:\n","\n","Traceback (most recent call last):\n","  File \"/tmp/ipykernel_19/1602423171.py\", line 1128, in <cell line: 0>\n","    output_path, fold_splits = main_preprocessing(\n","                               ^^^^^^^^^^^^^^^^^^^\n","  File \"/tmp/ipykernel_19/1602423171.py\", line 801, in main_preprocessing\n","    valid_indices = validate_images_fast(df_matched, min_size=config.min_image_size)\n","                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/tmp/ipykernel_19/1602423171.py\", line 304, in validate_images_fast\n","    img_name = df.loc[idx, 'name']\n","               ~~~~~~^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/pandas/core/indexing.py\", line 1183, in __getitem__\n","    return self.obj._get_value(*key, takeable=self._takeable)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\", line 4221, in _get_value\n","    row = self.index.get_loc(index)\n","          ^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\", line 3812, in get_loc\n","    raise KeyError(key) from err\n","KeyError: 1288\n"]}],"source":["import numpy as np\n","import pandas as pd\n","import os\n","import matplotlib\n","matplotlib.use('Agg')\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import KFold, StratifiedKFold\n","from sklearn.preprocessing import LabelEncoder, StandardScaler, RobustScaler\n","from tqdm.auto import tqdm\n","import warnings\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms\n","from PIL import Image\n","import json\n","from datetime import datetime\n","from scipy import stats\n","warnings.filterwarnings('ignore')\n","\n","\n","# ==================== VIT IMAGE PREPROCESSING CONFIGURATION ====================\n","class ViTPreprocessingConfig:\n","    \"\"\"Configuration for ViT preprocessing with multi-modal features\"\"\"\n","    def __init__(self):\n","        # Image settings\n","        self.image_size = 224\n","        self.channels = 3\n","        \n","        # ImageNet normalization\n","        self.mean = [0.485, 0.456, 0.406]\n","        self.std = [0.229, 0.224, 0.225]\n","        \n","        # Data augmentation settings\n","        self.horizontal_flip_prob = 0.5\n","        self.color_jitter = {\n","            'brightness': 0.2,\n","            'contrast': 0.2,\n","            'saturation': 0.1,\n","            'hue': 0.05\n","        }\n","        self.rotation_degrees = 10\n","        \n","        # Cross-validation\n","        self.n_folds = 5\n","        self.random_state = 42\n","        \n","        # BMI filtering\n","        self.bmi_range = (16, 45)\n","        \n","        # Image quality thresholds\n","        self.min_image_size = 1\n","        \n","        # Feature columns to use (exclude height/weight derivatives to prevent leakage)\n","        self.feature_columns = ['race', 'eyes', 'sex', 'hair', 'age']\n","        \n","        # Feature engineering settings\n","        self.reference_date = datetime(2025, 1, 1)\n","        self.age_bins = [0, 18, 25, 35, 45, 55, 65, 100]\n","        self.age_labels = ['<18', '18-24', '25-34', '35-44', '45-54', '55-64', '65+']\n","\n","\n","# ==================== BASIC FEATURE ENGINEERING (NO LEAKAGE) ====================\n","\n","def calculate_age(birth_date, reference_date=None):\n","    \"\"\"Calculate age from date of birth\"\"\"\n","    if pd.isna(birth_date):\n","        return np.nan\n","    \n","    if reference_date is None:\n","        reference_date = datetime.now()\n","    \n","    try:\n","        if isinstance(birth_date, str):\n","            birth_date = pd.to_datetime(birth_date, errors='coerce')\n","        \n","        if pd.isna(birth_date):\n","            return np.nan\n","            \n","        age = reference_date.year - birth_date.year\n","        \n","        if reference_date.month < birth_date.month or \\\n","           (reference_date.month == birth_date.month and reference_date.day < birth_date.day):\n","            age -= 1\n","            \n","        return age if age >= 0 else np.nan\n","    except:\n","        return np.nan\n","\n","\n","def calculate_bmi(height_inches, weight_lbs):\n","    \"\"\"Calculate BMI from height (inches) and weight (pounds) - used only for target\"\"\"\n","    if pd.isna(height_inches) or pd.isna(weight_lbs):\n","        return np.nan\n","    if height_inches <= 0 or weight_lbs <= 0:\n","        return np.nan\n","    \n","    height_m = height_inches * 0.0254\n","    weight_kg = weight_lbs * 0.453592\n","    bmi = weight_kg / (height_m ** 2)\n","    return bmi\n","\n","\n","def engineer_basic_features(df, config):\n","    \"\"\"\n","    Engineer ONLY basic features that don't depend on dataset statistics or height/weight.\n","    Height and weight are used ONLY to compute the target BMI.\n","    NO height/weight-derived features to prevent trivial prediction/leakage.\n","    \"\"\"\n","    df = df.copy()\n","    \n","    print(\"[1/3] Engineering demographic features...\")\n","    # Calculate age\n","    birth_col = next((col for col in ['birth', 'dob', 'date_of_birth'] if col in df.columns), None)\n","    if birth_col:\n","        df['age'] = df[birth_col].apply(lambda x: calculate_age(x, config.reference_date))\n","        \n","        # Age groups\n","        df['age_group'] = pd.cut(df['age'], bins=config.age_bins, labels=config.age_labels, right=False)\n","        \n","        # Age transformations (independent)\n","        df['age_squared'] = df['age'] ** 2\n","        df['age_decade'] = (df['age'] // 10) * 10\n","    \n","    print(\"[2/3] Cleaning categorical features...\")\n","    # Clean categorical features\n","    categorical_features = ['race', 'eyes', 'sex', 'hair']\n","    for feat in categorical_features:\n","        if feat in df.columns:\n","            df[feat] = clean_categorical_feature(df[feat])\n","    \n","    # Categorical interactions (no height/weight)\n","    if 'race' in df.columns and 'sex' in df.columns:\n","        df['race_sex'] = df['race'].astype(str) + '_' + df['sex'].astype(str)\n","    \n","    if 'race' in df.columns and 'age_group' in df.columns:\n","        df['race_age'] = df['race'].astype(str) + '_' + df['age_group'].astype(str)\n","    \n","    if 'eyes' in df.columns and 'hair' in df.columns:\n","        df['eye_hair_combo'] = df['eyes'].astype(str) + '_' + df['hair'].astype(str)\n","    \n","    print(\"[3/3] Engineering image features...\")\n","    # Image path features\n","    if 'image_path' in df.columns:\n","        df['image_type'] = df['image_path'].apply(\n","            lambda x: 'front' if 'front' in str(x).lower() \n","            else ('side' if 'side' in str(x).lower() \n","            else ('inmates' if 'inmates' in str(x).lower() else 'unknown'))\n","        )\n","        df['has_front_image'] = (df['image_type'] == 'front').astype(int)\n","        df['has_side_image'] = (df['image_type'] == 'side').astype(int)\n","    \n","    # Compute target BMI (but do not create any features from height/weight)\n","    if 'height' in df.columns and 'weight' in df.columns:\n","        df['bmi'] = df.apply(lambda row: calculate_bmi(row['height'], row['weight']), axis=1)\n","    \n","    # Remove height and weight columns to prevent accidental usage\n","    df = df.drop(columns=['height', 'weight'], errors='ignore')\n","    \n","    # Remove rows with invalid BMI\n","    initial_count = len(df)\n","    df = df.dropna(subset=['bmi'])\n","    print(f\"\\nRemoved {initial_count - len(df)} rows with invalid BMI\")\n","    print(f\"Final shape after basic feature engineering: {df.shape}\")\n","    \n","    return df\n","\n","\n","def engineer_statistical_features(df, reference_stats=None, fit=True):\n","    \"\"\"\n","    Calculate statistical features (z-scores, percentiles, etc.)\n","    MUST be called separately for train/val to avoid leakage.\n","    Only apply to non-leakage features like age.\n","    \"\"\"\n","    df = df.copy()\n","    \n","    # ONLY use age (no height/weight to prevent leakage)\n","    numeric_cols = ['age']\n","    existing_cols = [col for col in numeric_cols if col in df.columns]\n","    \n","    if reference_stats is None:\n","        reference_stats = {}\n","    \n","    for col in existing_cols:\n","        if df[col].notna().sum() == 0:\n","            continue\n","        \n","        if fit:\n","            # Calculate statistics from THIS data (training data)\n","            col_data = df[col].dropna()\n","            stats_dict = {\n","                'mean': float(col_data.mean()),\n","                'std': float(col_data.std()),\n","                'median': float(col_data.median()),\n","                'q25': float(col_data.quantile(0.25)),\n","                'q75': float(col_data.quantile(0.75))\n","            }\n","            reference_stats[col] = stats_dict\n","        else:\n","            # Use pre-calculated statistics (from training data)\n","            if col not in reference_stats:\n","                print(f\"Warning: No reference stats for {col}, skipping\")\n","                continue\n","            stats_dict = reference_stats[col]\n","        \n","        # Apply transformations using appropriate statistics\n","        mean = stats_dict['mean']\n","        std = stats_dict['std']\n","        median = stats_dict['median']\n","        \n","        # Z-score normalization\n","        df[f'{col}_zscore'] = (df[col] - mean) / (std + 1e-8)\n","        \n","        # Deviation from median\n","        df[f'{col}_deviation_from_median'] = df[col] - median\n","        \n","        # Binary: above/below mean\n","        df[f'{col}_above_mean'] = (df[col] > mean).astype(int)\n","        \n","        # Percentile rank (calculate on current data, this is okay)\n","        df[f'{col}_percentile'] = df[col].rank(pct=True) * 100\n","    \n","    return df, reference_stats\n","\n","\n","def clean_categorical_feature(series, valid_values=None):\n","    \"\"\"Clean and standardize categorical features\"\"\"\n","    series = series.astype(str).str.strip().str.upper()\n","    series = series.replace(['', 'NAN', 'NONE', 'NULL', 'UNKNOWN', 'NOT AVAILABLE', 'VOID'], 'UNKNOWN')\n","    \n","    if valid_values is not None:\n","        series = series.apply(lambda x: x if x in valid_values else 'UNKNOWN')\n","    \n","    return series\n","\n","\n","def process_person_data(csv_path, config):\n","    \"\"\"\n","    Load person.csv and apply BASIC feature engineering only\n","    Statistical features will be added per-fold to prevent leakage\n","    \"\"\"\n","    df = pd.read_csv(csv_path, delimiter=';')\n","    \n","    print(\"=== Starting Basic Feature Engineering ===\")\n","    print(f\"Initial shape: {df.shape}\")\n","    \n","    # Apply basic feature engineering (no dataset statistics)\n","    df = engineer_basic_features(df, config)\n","    \n","    print(\"\\n=== Basic Feature Engineering Summary ===\")\n","    print(f\"Total features created: {df.shape[1]}\")\n","    \n","    return df\n","\n","\n","# ==================== DATA MATCHING AND VALIDATION ====================\n","\n","def match_images_to_data(df, image_dirs):\n","    \"\"\"Match images to person data\"\"\"\n","    if isinstance(image_dirs, str):\n","        image_dirs = [image_dirs]\n","    \n","    # Collect all available images\n","    available_images = {}\n","    for image_dir in image_dirs:\n","        if not os.path.exists(image_dir):\n","            continue\n","        for f in os.listdir(image_dir):\n","            if f.lower().endswith(('.jpg', '.jpeg', '.png')):\n","                stem = os.path.splitext(f)[0].lower()\n","                available_images[stem] = os.path.join(image_dir, f)\n","    \n","    id_columns = ['id', 'ID', 'person_id', 'subject_id', 'doc_id', 'inmate_id']\n","    id_col = None\n","    for col in id_columns:\n","        if col in df.columns:\n","            id_col = col\n","            break\n","    \n","    if id_col is None:\n","        id_col = df.columns[0]\n","    \n","    df['image_stem'] = df[id_col].astype(str).str.lower()\n","    df['has_image'] = df['image_stem'].isin(available_images.keys())\n","    df['image_path'] = df['image_stem'].apply(lambda x: available_images.get(x, None))\n","    \n","    matched = df['has_image'].sum()\n","    print(f\"\\nMatched {matched}/{len(df)} records to images ({matched/len(df)*100:.1f}%)\")\n","    \n","    df_matched = df[df['has_image']].copy()\n","    df_matched = df_matched.drop(columns=['name'], errors='ignore')\n","    df_matched = df_matched.rename(columns={'image_stem': 'name'})\n","    \n","    return df_matched\n","\n","\n","def validate_images_fast(df, min_size=1):\n","    \"\"\"Fast image validation using basic checks only\"\"\"\n","    valid_indices = []\n","    failed_images = []\n","    \n","    print(f\"Fast validating {len(df)} images...\")\n","    \n","    for idx in tqdm(range(len(df)), desc=\"Quick validation\"):\n","        img_name = df.loc[idx, 'name']\n","        image_path = df.loc[idx, 'image_path']\n","        \n","        if pd.isna(image_path) or not os.path.exists(image_path):\n","            failed_images.append((idx, img_name, \"Not found\"))\n","            continue\n","        \n","        # Quick file size check (much faster than loading)\n","        file_size = os.path.getsize(image_path)\n","        if file_size < 1000:  # Less than 1KB = likely corrupt\n","            failed_images.append((idx, img_name, \"Too small\"))\n","            continue\n","        \n","        valid_indices.append(idx)\n","    \n","    if failed_images and len(failed_images) < 100:\n","        failed_df = pd.DataFrame(failed_images, columns=['index', 'filename', 'reason'])\n","        failed_df.to_csv('failed_images_log.csv', index=False)\n","    \n","    print(f\"Validated {len(valid_indices)}/{len(df)} images\")\n","    return valid_indices\n","\n","\n","def filter_dataset(df, valid_indices, bmi_range=(16, 45)):\n","    \"\"\"Filter dataset based on valid images and BMI range\"\"\"\n","    df_filtered = df.iloc[valid_indices].copy()\n","    \n","    if 'bmi' in df_filtered.columns:\n","        initial_len = len(df_filtered)\n","        df_filtered = df_filtered[\n","            (df_filtered['bmi'] >= bmi_range[0]) & \n","            (df_filtered['bmi'] <= bmi_range[1])\n","        ].reset_index(drop=True)\n","        print(f\"Filtered {initial_len - len(df_filtered)} samples outside BMI range {bmi_range}\")\n","    \n","    return df_filtered\n","\n","\n","# ==================== K-FOLD CREATION (BEFORE STATISTICAL FEATURES) ====================\n","\n","def create_kfold_splits(df, config):\n","    \"\"\"\n","    Create K-fold splits BEFORE adding statistical features\n","    This is the key to preventing data leakage\n","    \"\"\"\n","    use_stratified = False\n","    \n","    if 'bmi' in df.columns and len(df) >= config.n_folds * 10:\n","        df_temp = df.copy()\n","        try:\n","            df_temp['bmi_quartile'] = pd.qcut(df_temp['bmi'], q=config.n_folds, \n","                                             labels=False, duplicates='drop')\n","            stratify_labels = df_temp['bmi_quartile'].values\n","            kf = StratifiedKFold(n_splits=config.n_folds, shuffle=True, \n","                                random_state=config.random_state)\n","            split_iterator = kf.split(df, stratify_labels)\n","            use_stratified = True\n","            print(f\"\\nUsing Stratified K-Fold with {config.n_folds} folds\")\n","        except:\n","            kf = KFold(n_splits=config.n_folds, shuffle=True, \n","                      random_state=config.random_state)\n","            split_iterator = kf.split(df)\n","            print(f\"\\nUsing K-Fold with {config.n_folds} folds\")\n","    else:\n","        kf = KFold(n_splits=config.n_folds, shuffle=True, \n","                  random_state=config.random_state)\n","        split_iterator = kf.split(df)\n","        print(f\"\\nUsing K-Fold with {config.n_folds} folds\")\n","    \n","    fold_splits = []\n","    for fold, (train_idx, val_idx) in enumerate(split_iterator):\n","        train_df = df.iloc[train_idx].reset_index(drop=True)\n","        val_df = df.iloc[val_idx].reset_index(drop=True)\n","        \n","        # Verify no data leakage\n","        train_names = set(train_df['name'].astype(str))\n","        val_names = set(val_df['name'].astype(str))\n","        assert len(train_names & val_names) == 0, f\"Data leakage detected in fold {fold+1}!\"\n","        \n","        fold_info = {\n","            'fold': fold + 1,\n","            'train_size': len(train_df),\n","            'val_size': len(val_df),\n","            'train_df': train_df,\n","            'val_df': val_df\n","        }\n","        \n","        if 'bmi' in df.columns:\n","            fold_info['train_bmi_stats'] = {\n","                'mean': float(train_df['bmi'].mean()),\n","                'std': float(train_df['bmi'].std()),\n","                'min': float(train_df['bmi'].min()),\n","                'max': float(train_df['bmi'].max())\n","            }\n","            fold_info['val_bmi_stats'] = {\n","                'mean': float(val_df['bmi'].mean()),\n","                'std': float(val_df['bmi'].std()),\n","                'min': float(val_df['bmi'].min()),\n","                'max': float(val_df['bmi'].max())\n","            }\n","        \n","        fold_splits.append(fold_info)\n","        \n","        print(f\"  Fold {fold+1}: Train={len(train_df)}, Val={len(val_df)}\")\n","    \n","    return fold_splits\n","\n","\n","# ==================== NORMALIZATION (PER-FOLD) ====================\n","\n","def normalize_features(df, feature_encoders=None, scalers=None, fit=True):\n","    \"\"\"\n","    Normalize categorical and numerical features\n","    MUST be called separately for train/val with fit=True/False\n","    \"\"\"\n","    df = df.copy()\n","    \n","    if feature_encoders is None:\n","        feature_encoders = {}\n","    if scalers is None:\n","        scalers = {}\n","    \n","    # Categorical encoding\n","    categorical_cols = [col for col in df.columns if df[col].dtype == 'object']\n","    \n","    for col in categorical_cols:\n","        if col in ['name', 'image_path', 'image_stem']:\n","            continue\n","            \n","        if fit:\n","            le = LabelEncoder()\n","            try:\n","                df[f'{col}_encoded'] = le.fit_transform(df[col].fillna('UNKNOWN'))\n","                feature_encoders[col] = le\n","            except:\n","                pass\n","        else:\n","            if col in feature_encoders:\n","                le = feature_encoders[col]\n","                df[f'{col}_encoded'] = df[col].fillna('UNKNOWN').apply(\n","                    lambda x: le.transform([x])[0] if x in le.classes_ else -1\n","                )\n","    \n","    # Numerical scaling (exclude any potential leakage columns)\n","    numerical_cols = df.select_dtypes(include=[np.number]).columns\n","    scale_cols = [col for col in numerical_cols if not col.endswith('_encoded') and \n","                  col not in ['name', 'image_path', 'bmi'] and \n","                  not col.startswith('is_') and not col.startswith('has_') and \n","                  'height' not in col and 'weight' not in col]\n","    \n","    if fit and len(scale_cols) > 0:\n","        scaler = RobustScaler()\n","        df[scale_cols] = scaler.fit_transform(df[scale_cols].fillna(df[scale_cols].median()))\n","        scalers['numerical'] = scaler\n","    elif 'numerical' in scalers and len(scale_cols) > 0:\n","        scaler = scalers['numerical']\n","        df[scale_cols] = scaler.transform(df[scale_cols].fillna(df[scale_cols].median()))\n","    \n","    return df, feature_encoders, scalers\n","\n","\n","# ==================== SAVE PREPROCESSED DATA ====================\n","\n","def save_preprocessed_data(fold_splits, config, image_dirs, output_dir='illinois_doc_preprocessed'):\n","    \"\"\"Save preprocessed fold splits with per-fold normalization\"\"\"\n","    os.makedirs(output_dir, exist_ok=True)\n","    \n","    print(f\"\\n=== Processing and Saving Folds to {output_dir} ===\")\n","    \n","    # We'll store one set of encoders (they should be similar across folds)\n","    global_feature_encoders = {}\n","    \n","    for fold_info in fold_splits:\n","        fold_num = fold_info['fold']\n","        fold_dir = f'{output_dir}/fold_{fold_num}'\n","        os.makedirs(fold_dir, exist_ok=True)\n","        \n","        print(f\"\\nProcessing Fold {fold_num}...\")\n","        \n","        train_df = fold_info['train_df'].copy()\n","        val_df = fold_info['val_df'].copy()\n","        \n","        # Add statistical features (FIT on train, TRANSFORM on val)\n","        print(f\"  Adding statistical features (fit on train)...\")\n","        train_df, train_stats = engineer_statistical_features(train_df, reference_stats=None, fit=True)\n","        val_df, _ = engineer_statistical_features(val_df, reference_stats=train_stats, fit=False)\n","        \n","        # Normalize features (FIT on train, TRANSFORM on val)\n","        print(f\"  Normalizing features (fit on train)...\")\n","        train_df, feature_encoders, scalers = normalize_features(train_df, fit=True)\n","        val_df, _, _ = normalize_features(val_df, feature_encoders, scalers, fit=False)\n","        \n","        # Store encoders from first fold\n","        if fold_num == 1:\n","            global_feature_encoders = feature_encoders\n","        \n","        # Save fold data\n","        train_df.to_csv(f'{fold_dir}/train.csv', index=False)\n","        val_df.to_csv(f'{fold_dir}/val.csv', index=False)\n","        \n","        # Save fold-specific statistics\n","        fold_metadata = {\n","            'fold': fold_num,\n","            'train_size': len(train_df),\n","            'val_size': len(val_df),\n","            'feature_count': len(train_df.columns),\n","            'train_bmi_stats': fold_info.get('train_bmi_stats', {}),\n","            'val_bmi_stats': fold_info.get('val_bmi_stats', {}),\n","            'reference_stats': train_stats\n","        }\n","        \n","        with open(f'{fold_dir}/metadata.json', 'w') as f:\n","            json.dump(fold_metadata, f, indent=4)\n","        \n","        print(f\"  ✓ Fold {fold_num} saved: {len(train_df)} train, {len(val_df)} val samples\")\n","    \n","    # Save global configuration\n","    config_dict = {\n","        'image_size': config.image_size,\n","        'channels': config.channels,\n","        'mean': config.mean,\n","        'std': config.std,\n","        'n_folds': config.n_folds,\n","        'bmi_range': list(config.bmi_range),\n","        'image_dirs': image_dirs,\n","        'random_state': config.random_state,\n","        'feature_columns': config.feature_columns,\n","        'augmentation': {\n","            'horizontal_flip_prob': config.horizontal_flip_prob,\n","            'color_jitter': config.color_jitter,\n","            'rotation_degrees': config.rotation_degrees\n","        },\n","        'age_bins': config.age_bins,\n","        'age_labels': config.age_labels\n","    }\n","    \n","    with open(f'{output_dir}/config.json', 'w') as f:\n","        json.dump(config_dict, f, indent=4)\n","    \n","    # Save feature encoders (from first fold as reference)\n","    encoder_dict = {}\n","    for feature, encoder in global_feature_encoders.items():\n","        encoder_dict[feature] = {\n","            'classes': encoder.classes_.tolist()\n","        }\n","    \n","    with open(f'{output_dir}/feature_encoders.json', 'w') as f:\n","        json.dump(encoder_dict, f, indent=4)\n","    \n","    # Create summary\n","    sample_train_df = pd.read_csv(f'{output_dir}/fold_1/train.csv')\n","    all_features = list(sample_train_df.columns)\n","    \n","    summary = {\n","        'total_folds': config.n_folds,\n","        'total_samples': fold_splits[0]['train_size'] + fold_splits[0]['val_size'],\n","        'total_features': len(all_features),\n","        'image_size': config.image_size,\n","        'normalization': {'mean': config.mean, 'std': config.std},\n","        'feature_columns': all_features,\n","        'categorical_features': [col for col in all_features if col.endswith('_encoded')],\n","        'numerical_features': [col for col in all_features if any(x in col for x in \n","                              ['_zscore', '_percentile', '_squared', '_deviation', '_interaction'])],\n","        'fold_summary': [\n","            {\n","                'fold': f['fold'],\n","                'train_size': f['train_size'],\n","                'val_size': f['val_size'],\n","                'train_bmi_mean': f.get('train_bmi_stats', {}).get('mean', 0),\n","                'val_bmi_mean': f.get('val_bmi_stats', {}).get('mean', 0)\n","            }\n","            for f in fold_splits\n","        ]\n","    }\n","    \n","    with open(f'{output_dir}/summary.json', 'w') as f:\n","        json.dump(summary, f, indent=4)\n","    \n","    print(f\"\\n✓ All folds saved successfully\")\n","    print(f\"✓ Total features: {len(all_features)}\")\n","    print(f\"✓ Categorical features: {len(summary['categorical_features'])}\")\n","    print(f\"✓ Numerical features: {len(summary['numerical_features'])}\")\n","    \n","    return output_dir\n","\n","\n","# ==================== DATA QUALITY & ANALYSIS ====================\n","\n","def check_data_leakage(output_dir):\n","    \"\"\"Verify no data leakage between train/val\"\"\"\n","    print(\"\\n=== Checking for Data Leakage ===\")\n","    \n","    issues_found = False\n","    \n","    # Load first fold\n","    train_df = pd.read_csv(f'{output_dir}/fold_1/train.csv')\n","    \n","    # Check for BMI-derived features\n","    bmi_features = [col for col in train_df.columns if 'bmi' in col.lower() and col != 'bmi']\n","    if bmi_features:\n","        print(f\"⚠️  WARNING: BMI-derived features found: {bmi_features}\")\n","        issues_found = True\n","    \n","    # Check for height/weight features\n","    hw_features = [col for col in train_df.columns if 'height' in col.lower() or 'weight' in col.lower() or 'bsa' in col.lower() or 'ponderal' in col.lower()]\n","    if hw_features:\n","        print(f\"⚠️  WARNING: Height/weight-derived features found: {hw_features}\")\n","        issues_found = True\n","    \n","    # Check correlations with BMI\n","    print(\"\\nChecking feature correlations with BMI:\")\n","    numeric_cols = train_df.select_dtypes(include=[np.number]).columns\n","    high_corr_features = []\n","    \n","    for col in numeric_cols:\n","        if col != 'bmi' and train_df[col].notna().sum() > 0:\n","            try:\n","                corr = train_df[[col, 'bmi']].corr().iloc[0, 1]\n","                if abs(corr) > 0.95:\n","                    print(f\"  ⚠️  {col}: correlation = {corr:.4f}\")\n","                    high_corr_features.append((col, corr))\n","                    issues_found = True\n","            except:\n","                pass\n","    \n","    if not issues_found:\n","        print(\"  ✓ No data leakage detected!\")\n","        print(\"  ✓ No BMI-derived features found\")\n","        print(\"  ✓ No height/weight-derived features found\")\n","        print(\"  ✓ No suspiciously high correlations with BMI\")\n","    \n","    return not issues_found\n","\n","\n","def analyze_dataset(df, output_dir):\n","    \"\"\"Create analysis plots\"\"\"\n","    os.makedirs(output_dir, exist_ok=True)\n","    \n","    fig = plt.figure(figsize=(20, 10))\n","    \n","    # BMI Distribution\n","    plt.subplot(2, 4, 1)\n","    plt.hist(df['bmi'], bins=50, alpha=0.7, edgecolor='black', color='steelblue')\n","    plt.xlabel('BMI', fontsize=11)\n","    plt.ylabel('Frequency', fontsize=11)\n","    plt.title('BMI Distribution', fontsize=12, fontweight='bold')\n","    plt.grid(True, alpha=0.3)\n","    \n","    # Age Distribution\n","    plt.subplot(2, 4, 2)\n","    if 'age' in df.columns:\n","        plt.hist(df['age'].dropna(), bins=30, alpha=0.7, edgecolor='black', color='coral')\n","        plt.xlabel('Age', fontsize=11)\n","        plt.ylabel('Frequency', fontsize=11)\n","        plt.title('Age Distribution', fontsize=12, fontweight='bold')\n","        plt.grid(True, alpha=0.3)\n","    \n","    # BMI vs Age\n","    plt.subplot(2, 4, 3)\n","    \n","    if 'age' in df.columns:\n","        plt.scatter(df['age'], df['bmi'], alpha=0.3, s=10)\n","        plt.xlabel('Age', fontsize=11)\n","        plt.ylabel('BMI', fontsize=11)\n","        plt.title('BMI vs Age', fontsize=12, fontweight='bold')\n","        plt.grid(True, alpha=0.3)\n","    \n","    # Removed Height vs Weight plot to avoid leakage visualization\n","    \n","    # Sex Distribution\n","    plt.subplot(2, 4, 5)\n","    if 'sex' in df.columns:\n","        sex_counts = df['sex'].value_counts()\n","        sex_counts.plot(kind='bar', color=['lightblue', 'lightpink'], edgecolor='black')\n","        plt.xlabel('Sex', fontsize=11)\n","        plt.ylabel('Count', fontsize=11)\n","        plt.title('Sex Distribution', fontsize=12, fontweight='bold')\n","        plt.xticks(rotation=0)\n","        plt.grid(True, alpha=0.3, axis='y')\n","    \n","    # Race Distribution\n","    plt.subplot(2, 4, 6)\n","    if 'race' in df.columns:\n","        race_counts = df['race'].value_counts().head(10)\n","        race_counts.plot(kind='barh', color='lightgreen', edgecolor='black')\n","        plt.xlabel('Count', fontsize=11)\n","        plt.ylabel('Race', fontsize=11)\n","        plt.title('Top 10 Race Categories', fontsize=12, fontweight='bold')\n","        plt.grid(True, alpha=0.3, axis='x')\n","    \n","    # BMI Statistics by Sex\n","    plt.subplot(2, 4, 7)\n","    if 'sex' in df.columns and 'bmi' in df.columns:\n","        df.boxplot(column='bmi', by='sex', ax=plt.gca())\n","        plt.xlabel('Sex', fontsize=11)\n","        plt.ylabel('BMI', fontsize=11)\n","        plt.title('BMI Distribution by Sex', fontsize=12, fontweight='bold')\n","        plt.suptitle('')  # Remove automatic title\n","        plt.grid(True, alpha=0.3)\n","    \n","    # Statistics Summary\n","    plt.subplot(2, 4, 8)\n","    stats_text = f\"\"\"\n","    Dataset Statistics:\n","    \n","    Total Samples: {len(df):,}\n","    \n","    BMI:\n","      Mean: {df['bmi'].mean():.2f}\n","      Std: {df['bmi'].std():.2f}\n","      Min: {df['bmi'].min():.2f}\n","      Max: {df['bmi'].max():.2f}\n","    \"\"\"\n","    \n","    if 'age' in df.columns:\n","        stats_text += f\"\"\"\n","    Age:\n","      Mean: {df['age'].mean():.1f}\n","      Range: {df['age'].min():.0f}-{df['age'].max():.0f}\n","    \"\"\"\n","    \n","    plt.text(0.1, 0.5, stats_text, fontsize=10, family='monospace',\n","            verticalalignment='center', transform=plt.gca().transAxes)\n","    plt.axis('off')\n","    plt.title('Summary Statistics', fontsize=12, fontweight='bold')\n","    \n","    plt.tight_layout()\n","    plt.savefig(f'{output_dir}/dataset_analysis.png', dpi=150, bbox_inches='tight')\n","    plt.close()\n","    \n","    print(f\"✓ Dataset analysis saved to {output_dir}/dataset_analysis.png\")\n","\n","\n","def print_feature_summary(df):\n","    \"\"\"Print summary of engineered features\"\"\"\n","    print(\"\\n=== Feature Summary ===\")\n","    \n","    all_features = df.columns.tolist()\n","    \n","    feature_categories = {\n","        'Age Features': [c for c in all_features if 'age' in c and 'bmi' not in c],\n","        'Statistical Features': [c for c in all_features if any(x in c for x in ['zscore', 'percentile', 'deviation', 'above_mean'])],\n","        'Categorical': [c for c in all_features if any(x in c for x in ['race', 'sex', 'eyes', 'hair', 'combo'])],\n","        'Image Features': [c for c in all_features if any(x in c for x in ['image_type', 'has_front', 'has_side'])],\n","    }\n","    \n","    for category, features in feature_categories.items():\n","        if features:\n","            print(f\"\\n{category}:\")\n","            for feat in features[:10]:  # Show first 10\n","                print(f\"  - {feat}\")\n","            if len(features) > 10:\n","                print(f\"  ... and {len(features) - 10} more\")\n","    \n","    print(f\"\\nTotal features: {len(all_features)}\")\n","    \n","    # Check for problematic features\n","    problematic = [f for f in all_features if 'bmi' in f.lower() and f != 'bmi']\n","    if problematic:\n","        print(f\"\\n⚠️  WARNING: Found {len(problematic)} BMI-derived features:\")\n","        for f in problematic:\n","            print(f\"  - {f}\")\n","    \n","    hw_problematic = [f for f in all_features if 'height' in f.lower() or 'weight' in f.lower() or 'bsa' in f.lower() or 'ponderal' in f.lower()]\n","    if hw_problematic:\n","        print(f\"\\n⚠️  WARNING: Found {len(hw_problematic)} height/weight-derived features:\")\n","        for f in hw_problematic:\n","            print(f\"  - {f}\")\n","\n","\n","# ==================== MAIN PREPROCESSING PIPELINE ====================\n","\n","def main_preprocessing(person_csv_path, image_dirs, output_dir='illinois_doc_preprocessed', show_plots=True):\n","    \"\"\"\n","    Complete preprocessing pipeline WITHOUT data leakage\n","    Statistical features are calculated per-fold\n","    \"\"\"\n","    \n","    print(\"=\" * 70)\n","    print(\"  ILLINOIS DOC DATASET - LEAKAGE-FREE PREPROCESSING PIPELINE\")\n","    print(\"=\" * 70)\n","    \n","    config = ViTPreprocessingConfig()\n","    \n","    # Step 1: Load and engineer BASIC features only\n","    print(\"\\n[STEP 1] Loading data and engineering basic features...\")\n","    df = process_person_data(person_csv_path, config)\n","    \n","    # Step 2: Match images\n","    print(\"\\n[STEP 2] Matching images to data...\")\n","    df_matched = match_images_to_data(df, image_dirs)\n","    \n","    if len(df_matched) == 0:\n","        raise ValueError(\"No matching images found!\")\n","    \n","    # Step 3: Validate images\n","    print(\"\\n[STEP 3] Validating images...\")\n","    valid_indices = validate_images_fast(df_matched, min_size=config.min_image_size)\n","    \n","    if len(valid_indices) == 0:\n","        raise ValueError(\"No valid images found! Check failed_images_log.csv for details.\")\n","    \n","    # Step 4: Filter by BMI range\n","    print(\"\\n[STEP 4] Filtering dataset by BMI range...\")\n","    df_filtered = filter_dataset(df_matched, valid_indices, config.bmi_range)\n","    \n","    if len(df_filtered) < config.n_folds * 2:\n","        raise ValueError(f\"Not enough samples ({len(df_filtered)}) for {config.n_folds}-fold CV\")\n","    \n","    print(f\"✓ Retained {len(df_filtered)} samples within BMI range {config.bmi_range}\")\n","    \n","    # Step 5: Analyze dataset\n","    print(\"\\n[STEP 5] Analyzing dataset...\")\n","    if show_plots:\n","        analyze_dataset(df_filtered, '/kaggle/working')\n","    \n","    print_feature_summary(df_filtered)\n","    \n","    # Step 6: Create K-fold splits (BEFORE statistical features)\n","    print(\"\\n[STEP 6] Creating K-fold splits...\")\n","    fold_splits = create_kfold_splits(df_filtered, config)\n","    \n","    # Step 7: Save preprocessed data (adds statistical features per-fold)\n","    print(\"\\n[STEP 7] Adding per-fold statistical features and saving...\")\n","    output_path = save_preprocessed_data(fold_splits, config, image_dirs, output_dir)\n","    \n","    # Step 8: Verify no data leakage\n","    print(\"\\n[STEP 8] Verifying no data leakage...\")\n","    is_clean = check_data_leakage(output_path)\n","    \n","    print(\"\\n\" + \"=\" * 70)\n","    print(\"  PREPROCESSING COMPLETE!\")\n","    print(\"=\" * 70)\n","    print(f\"✓ Output directory: {output_path}\")\n","    print(f\"✓ Ready for training with {len(df_filtered)} samples\")\n","    print(f\"✓ BMI range: {df_filtered['bmi'].min():.2f} - {df_filtered['bmi'].max():.2f}\")\n","    print(f\"✓ No data leakage: {is_clean}\")\n","    print(\"=\" * 70 + \"\\n\")\n","    \n","    return output_path, fold_splits\n","\n","\n","# ==================== UTILITY FUNCTIONS ====================\n","\n","def load_config(preprocessed_dir='illinois_doc_preprocessed'):\n","    \"\"\"Load preprocessing configuration\"\"\"\n","    config_path = f'{preprocessed_dir}/config.json'\n","    \n","    if not os.path.exists(config_path):\n","        raise FileNotFoundError(f\"Configuration file not found: {config_path}\")\n","    \n","    with open(config_path, 'r') as f:\n","        config_dict = json.load(f)\n","    \n","    config = ViTPreprocessingConfig()\n","    config.image_size = config_dict['image_size']\n","    config.mean = config_dict['mean']\n","    config.std = config_dict['std']\n","    config.n_folds = config_dict['n_folds']\n","    config.bmi_range = tuple(config_dict['bmi_range'])\n","    config.feature_columns = config_dict.get('feature_columns', [])\n","    \n","    if 'augmentation' in config_dict:\n","        aug = config_dict['augmentation']\n","        config.horizontal_flip_prob = aug.get('horizontal_flip_prob', 0.5)\n","        config.color_jitter = aug.get('color_jitter', config.color_jitter)\n","        config.rotation_degrees = aug.get('rotation_degrees', 10)\n","    \n","    if 'age_bins' in config_dict:\n","        config.age_bins = config_dict['age_bins']\n","        config.age_labels = config_dict['age_labels']\n","    \n","    image_dirs = config_dict.get('image_dirs', [config_dict.get('image_dir', '')])\n","    \n","    return config, image_dirs\n","\n","\n","def load_feature_encoders(preprocessed_dir='illinois_doc_preprocessed'):\n","    \"\"\"Load feature encoders\"\"\"\n","    encoder_path = f'{preprocessed_dir}/feature_encoders.json'\n","    \n","    if not os.path.exists(encoder_path):\n","        return {}\n","    \n","    with open(encoder_path, 'r') as f:\n","        encoder_dict = json.load(f)\n","    \n","    feature_encoders = {}\n","    for feature, info in encoder_dict.items():\n","        le = LabelEncoder()\n","        le.classes_ = np.array(info['classes'])\n","        feature_encoders[feature] = le\n","    \n","    return feature_encoders\n","\n","\n","def verify_preprocessing(preprocessed_dir='illinois_doc_preprocessed'):\n","    \"\"\"Verify preprocessing was successful\"\"\"\n","    \n","    print(f\"\\n=== Verifying Preprocessing: {preprocessed_dir} ===\")\n","    \n","    if not os.path.exists(preprocessed_dir):\n","        print(\"✗ Preprocessed directory not found\")\n","        return False\n","    \n","    if not os.path.exists(f'{preprocessed_dir}/config.json'):\n","        print(\"✗ Configuration file not found\")\n","        return False\n","    \n","    if not os.path.exists(f'{preprocessed_dir}/feature_encoders.json'):\n","        print(\"✗ Feature encoders file not found\")\n","        return False\n","    \n","    print(\"✓ Core files exist\")\n","    \n","    config, image_dirs = load_config(preprocessed_dir)\n","    feature_encoders = load_feature_encoders(preprocessed_dir)\n","    \n","    print(f\"✓ Configuration loaded: {config.n_folds} folds\")\n","    print(f\"✓ Feature encoders loaded: {len(feature_encoders)} encoders\")\n","    \n","    all_folds_ok = True\n","    for fold in range(1, config.n_folds + 1):\n","        fold_dir = f'{preprocessed_dir}/fold_{fold}'\n","        if not os.path.exists(fold_dir):\n","            print(f\"✗ Fold {fold} directory not found\")\n","            all_folds_ok = False\n","            continue\n","        \n","        required_files = ['train.csv', 'val.csv', 'metadata.json']\n","        for file in required_files:\n","            if not os.path.exists(f'{fold_dir}/{file}'):\n","                print(f\"✗ Fold {fold}: {file} not found\")\n","                all_folds_ok = False\n","    \n","    if all_folds_ok:\n","        print(f\"✓ All {config.n_folds} folds verified\")\n","    \n","    # Load summary\n","    if os.path.exists(f'{preprocessed_dir}/summary.json'):\n","        with open(f'{preprocessed_dir}/summary.json', 'r') as f:\n","            summary = json.load(f)\n","        print(f\"\\n=== Dataset Summary ===\")\n","        print(f\"Total samples: {summary['total_samples']}\")\n","        print(f\"Total features: {summary['total_features']}\")\n","        print(f\"Categorical features: {len(summary.get('categorical_features', []))}\")\n","        print(f\"Numerical features: {len(summary.get('numerical_features', []))}\")\n","    \n","    return all_folds_ok\n","\n","\n","def visualize_batch(dataloader, num_images=8, save_path='batch_visualization.png'):\n","    \"\"\"Visualize a batch of preprocessed images\"\"\"\n","    batch = next(iter(dataloader))\n","    images = batch['image'][:num_images]\n","    bmis = batch['bmi'][:num_images]\n","    \n","    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n","    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n","    \n","    fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n","    axes = axes.flatten()\n","    \n","    for idx in range(num_images):\n","        img = images[idx] * std + mean\n","        img = torch.clamp(img, 0, 1)\n","        img = img.permute(1, 2, 0).numpy()\n","        \n","        axes[idx].imshow(img)\n","        \n","        title = f'BMI: {bmis[idx].item():.2f}'\n","        \n","        if 'categorical_features' in batch:\n","            cat_feats = batch['categorical_features'][idx].tolist()\n","            title += f'\\nCat: {cat_feats[:3]}...'\n","        \n","        if 'numerical_features' in batch:\n","            num_feats = batch['numerical_features'][idx][:3].tolist()\n","            title += f'\\nNum: [{num_feats[0]:.2f}, {num_feats[1]:.2f}, ...]'\n","        \n","        axes[idx].set_title(title, fontsize=9, fontweight='bold')\n","        axes[idx].axis('off')\n","    \n","    plt.tight_layout()\n","    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n","    plt.close()\n","    print(f\"Batch visualization saved to {save_path}\")\n","\n","\n","# ==================== PYTORCH DATASET CLASS ====================\n","\n","class BMIDataset(Dataset):\n","    \"\"\"PyTorch Dataset for BMI prediction\"\"\"\n","    \n","    def __init__(self, dataframe, transform=None):\n","        self.df = dataframe.reset_index(drop=True)\n","        self.transform = transform\n","        \n","        # Identify feature columns\n","        self.categorical_cols = [col for col in self.df.columns if col.endswith('_encoded')]\n","        self.numerical_cols = [col for col in self.df.columns if \n","                              col.endswith(('_zscore', '_percentile', '_squared', \n","                                          '_interaction', '_deviation', '_product',\n","                                          '_above_mean')) and \n","                              self.df[col].dtype in [np.float32, np.float64, np.int32, np.int64] and\n","                              'height' not in col and 'weight' not in col]\n","    \n","    def __len__(self):\n","        return len(self.df)\n","    \n","    def __getitem__(self, idx):\n","        row = self.df.iloc[idx]\n","        \n","        # Load image\n","        img_path = row['image_path']\n","        try:\n","            with Image.open(img_path) as image:\n","                image = image.convert('RGB')\n","                if self.transform:\n","                    image = self.transform(image)\n","        except:\n","            # Return zero tensor if image fails\n","            image = torch.zeros(3, 224, 224)\n","        \n","        # Get BMI target\n","        bmi = torch.tensor(row['bmi'], dtype=torch.float32)\n","        \n","        # Get categorical features\n","        categorical_features = None\n","        if len(self.categorical_cols) > 0:\n","            categorical_features = torch.tensor([row[col] for col in self.categorical_cols], \n","                                                dtype=torch.long)\n","        \n","        # Get numerical features\n","        numerical_features = None\n","        if len(self.numerical_cols) > 0:\n","            numerical_features = torch.tensor([row[col] for col in self.numerical_cols], \n","                                             dtype=torch.float32)\n","        \n","        return {\n","            'image': image,\n","            'bmi': bmi,\n","            'categorical_features': categorical_features,\n","            'numerical_features': numerical_features,\n","            'image_name': row['name']\n","        }\n","\n","\n","def create_transforms(config_dict):\n","    \"\"\"Create data transforms\"\"\"\n","    train_transform = transforms.Compose([\n","        transforms.Resize((224, 224)),\n","        transforms.RandomHorizontalFlip(p=0.5),\n","        transforms.RandomRotation(degrees=10),\n","        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.1, hue=0.05),\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean=config_dict['mean'], std=config_dict['std'])\n","    ])\n","    \n","    val_transform = transforms.Compose([\n","        transforms.Resize((224, 224)),\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean=config_dict['mean'], std=config_dict['std'])\n","    ])\n","    \n","    return train_transform, val_transform\n","\n","\n","def create_dataloaders(fold_dir, config, batch_size=32, num_workers=4):\n","    \"\"\"Create PyTorch DataLoaders for a specific fold\"\"\"\n","    train_df = pd.read_csv(f'{fold_dir}/train.csv')\n","    val_df = pd.read_csv(f'{fold_dir}/val.csv')\n","    \n","    # Get config dict\n","    if isinstance(config, dict):\n","        config_dict = config\n","    else:\n","        config_dict = {\n","            'mean': config.mean,\n","            'std': config.std\n","        }\n","    \n","    train_transform, val_transform = create_transforms(config_dict)\n","    \n","    train_dataset = BMIDataset(train_df, transform=train_transform)\n","    val_dataset = BMIDataset(val_df, transform=val_transform)\n","    \n","    train_loader = DataLoader(\n","        train_dataset,\n","        batch_size=batch_size,\n","        shuffle=True,\n","        num_workers=num_workers,\n","        pin_memory=True,\n","        drop_last=False\n","    )\n","    \n","    val_loader = DataLoader(\n","        val_dataset,\n","        batch_size=batch_size,\n","        shuffle=False,\n","        num_workers=num_workers,\n","        pin_memory=True,\n","        drop_last=False\n","    )\n","    \n","    return train_loader, val_loader\n","\n","\n","# ==================== EXAMPLE USAGE ====================\n","if __name__ == \"__main__\":\n","    print(\"\\n\" + \"=\" * 70)\n","    print(\"  LEAKAGE-FREE ILLINOIS DOC PREPROCESSING\")\n","    print(\"=\" * 70 + \"\\n\")\n","    \n","    # Set paths for Illinois DOC dataset\n","    person_csv_path = '/kaggle/input/illinois-doc-labeled-faces-dataset/person.csv'\n","    image_dirs = [\n","        '/kaggle/input/illinois-doc-labeled-faces-dataset/front/front',\n","        '/kaggle/input/illinois-doc-labeled-faces-dataset/inmates/inmates',\n","        '/kaggle/input/illinois-doc-labeled-faces-dataset/side/side'\n","    ]\n","    \n","    # Run preprocessing\n","    try:\n","        output_path, fold_splits = main_preprocessing(\n","            person_csv_path, \n","            image_dirs, \n","            output_dir='/kaggle/working/illinois_doc_preprocessed',\n","            show_plots=False  # Set to True in local environment\n","        )\n","        \n","        # Verify preprocessing\n","        verify_preprocessing(output_path)\n","        \n","        # Test loading first fold\n","        print(\"\\n=== Testing Fold 1 DataLoader ===\")\n","        config, _ = load_config(output_path)\n","        train_loader, val_loader = create_dataloaders(\n","            f'{output_path}/fold_1', \n","            config, \n","            batch_size=16,\n","            num_workers=2\n","        )\n","        \n","        # Test one batch\n","        batch = next(iter(train_loader))\n","        print(f\"\\nBatch contents:\")\n","        print(f\"  - Images: {batch['image'].shape}\")\n","        print(f\"  - BMI: {batch['bmi'].shape}\")\n","        if batch['categorical_features'] is not None:\n","            print(f\"  - Categorical features: {batch['categorical_features'].shape}\")\n","        if batch['numerical_features'] is not None:\n","            print(f\"  - Numerical features: {batch['numerical_features'].shape}\")\n","        \n","        # Visualize batch\n","        visualize_batch(train_loader, num_images=8, \n","                       save_path='illinois_doc_batch_viz_clean.png')\n","        \n","        print(\"\\n\" + \"=\" * 70)\n","        print(\"  ALL PREPROCESSING STEPS COMPLETED SUCCESSFULLY!\")\n","        print(\"=\" * 70)\n","        print(f\"\\nYou can now use the preprocessed data for training:\")\n","        print(f\"  - Data location: {output_path}\")\n","        print(f\"  - Number of folds: {config.n_folds}\")\n","        \n","    except Exception as e:\n","        print(f\"\\n✗ Error during preprocessing: {str(e)}\")\n","        import traceback\n","        traceback.print_exc()"]},{"cell_type":"code","execution_count":2,"id":"b96ed5c3","metadata":{"execution":{"iopub.execute_input":"2025-10-19T02:57:44.847061Z","iopub.status.busy":"2025-10-19T02:57:44.846848Z","iopub.status.idle":"2025-10-19T02:58:16.05456Z","shell.execute_reply":"2025-10-19T02:58:16.053734Z"},"papermill":{"duration":31.224913,"end_time":"2025-10-19T02:58:16.06153","exception":false,"start_time":"2025-10-19T02:57:44.836617","status":"completed"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["2025-10-19 02:57:54.030749: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","E0000 00:00:1760842674.429687      19 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","E0000 00:00:1760842674.550505      19 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"]},{"name":"stdout","output_type":"stream","text":["\n","======================================================================\n","  TESTING MODEL CREATION\n","======================================================================\n","\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"805caf3560664bf1af30a747d3672f71","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/502 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1971425e75cb41f2a92a3a43d62d18fd","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\n","======================================================================\n","MODEL ARCHITECTURE - ViT for BMI Prediction\n","======================================================================\n","Visual Feature Dim:      768 (ViT-Base)\n","Pooling Strategy:        CLS Token\n","Categorical Features:    4 -> 128\n","Numerical Features:      15 -> 128\n","Fusion Dim:              512\n","Regression Head:         512 -> 256 -> 128 -> 1\n","Unfrozen ViT Layers:     Last 4\n","Dropout:                 0.2\n","======================================================================\n","\n","\n","Model Parameter Summary:\n","==================================================\n","Total Parameters:      88,236,193\n","Trainable Parameters:  30,198,433 (34.22%)\n","Frozen Parameters:     58,037,760 (65.78%)\n","==================================================\n","\n","Input shapes:\n","  Images: torch.Size([16, 3, 224, 224])\n","  Categorical: torch.Size([16, 4])\n","  Numerical: torch.Size([16, 15])\n","\n","Output shape: torch.Size([16, 1])\n","\n","✓ Model test passed!\n","\n","\n","======================================================================\n","  ViT FOR BMI PREDICTION - FIXED VERSION\n","======================================================================\n","\n","✓ Using GPU: Tesla T4\n","  Memory: 15.83 GB\n","\n","Configuration:\n","  Folds:              5\n","  Epochs:             20\n","  Batch size:         96\n","  Learning rate:      3e-05\n","  ViT LR multiplier:  0.1\n","  Loss function:      huber\n","  Mixed precision:    True\n","  Adaptive pooling:   False\n","  Output dir:         /kaggle/working/vit_bmi_checkpoints\n","  Log dir:            /kaggle/working/logs\n","\n","\n","======================================================================\n","TRAINING FOLD 1/5\n","======================================================================\n","\n","\n","✗ Error training fold 1: [Errno 2] No such file or directory: '/kaggle/working/illinois_doc_preprocessed/config.json'\n","\n","======================================================================\n","TRAINING FOLD 2/5\n","======================================================================\n","\n","\n","✗ Error training fold 2: [Errno 2] No such file or directory: '/kaggle/working/illinois_doc_preprocessed/config.json'\n","\n","======================================================================\n","TRAINING FOLD 3/5\n","======================================================================\n","\n","\n","✗ Error training fold 3: [Errno 2] No such file or directory: '/kaggle/working/illinois_doc_preprocessed/config.json'\n","\n","======================================================================\n","TRAINING FOLD 4/5\n","======================================================================\n","\n","\n","✗ Error training fold 4: [Errno 2] No such file or directory: '/kaggle/working/illinois_doc_preprocessed/config.json'\n","\n","======================================================================\n","TRAINING FOLD 5/5\n","======================================================================\n","\n","\n","✗ Error training fold 5: [Errno 2] No such file or directory: '/kaggle/working/illinois_doc_preprocessed/config.json'\n","\n","✗ No folds completed successfully!\n"]},{"name":"stderr","output_type":"stream","text":["Traceback (most recent call last):\n","  File \"/tmp/ipykernel_19/208876159.py\", line 1474, in main\n","    history = train_fold(fold, config, device)\n","              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/tmp/ipykernel_19/208876159.py\", line 1027, in train_fold\n","    preprocess_config, feature_encoders = load_config_and_encoders(config.preprocessed_dir)\n","                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/tmp/ipykernel_19/208876159.py\", line 690, in load_config_and_encoders\n","    with open(f'{preprocessed_dir}/config.json', 'r') as f:\n","         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","FileNotFoundError: [Errno 2] No such file or directory: '/kaggle/working/illinois_doc_preprocessed/config.json'\n","Traceback (most recent call last):\n","  File \"/tmp/ipykernel_19/208876159.py\", line 1474, in main\n","    history = train_fold(fold, config, device)\n","              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/tmp/ipykernel_19/208876159.py\", line 1027, in train_fold\n","    preprocess_config, feature_encoders = load_config_and_encoders(config.preprocessed_dir)\n","                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/tmp/ipykernel_19/208876159.py\", line 690, in load_config_and_encoders\n","    with open(f'{preprocessed_dir}/config.json', 'r') as f:\n","         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","FileNotFoundError: [Errno 2] No such file or directory: '/kaggle/working/illinois_doc_preprocessed/config.json'\n","Traceback (most recent call last):\n","  File \"/tmp/ipykernel_19/208876159.py\", line 1474, in main\n","    history = train_fold(fold, config, device)\n","              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/tmp/ipykernel_19/208876159.py\", line 1027, in train_fold\n","    preprocess_config, feature_encoders = load_config_and_encoders(config.preprocessed_dir)\n","                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/tmp/ipykernel_19/208876159.py\", line 690, in load_config_and_encoders\n","    with open(f'{preprocessed_dir}/config.json', 'r') as f:\n","         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","FileNotFoundError: [Errno 2] No such file or directory: '/kaggle/working/illinois_doc_preprocessed/config.json'\n","Traceback (most recent call last):\n","  File \"/tmp/ipykernel_19/208876159.py\", line 1474, in main\n","    history = train_fold(fold, config, device)\n","              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/tmp/ipykernel_19/208876159.py\", line 1027, in train_fold\n","    preprocess_config, feature_encoders = load_config_and_encoders(config.preprocessed_dir)\n","                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/tmp/ipykernel_19/208876159.py\", line 690, in load_config_and_encoders\n","    with open(f'{preprocessed_dir}/config.json', 'r') as f:\n","         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","FileNotFoundError: [Errno 2] No such file or directory: '/kaggle/working/illinois_doc_preprocessed/config.json'\n","Traceback (most recent call last):\n","  File \"/tmp/ipykernel_19/208876159.py\", line 1474, in main\n","    history = train_fold(fold, config, device)\n","              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/tmp/ipykernel_19/208876159.py\", line 1027, in train_fold\n","    preprocess_config, feature_encoders = load_config_and_encoders(config.preprocessed_dir)\n","                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/tmp/ipykernel_19/208876159.py\", line 690, in load_config_and_encoders\n","    with open(f'{preprocessed_dir}/config.json', 'r') as f:\n","         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","FileNotFoundError: [Errno 2] No such file or directory: '/kaggle/working/illinois_doc_preprocessed/config.json'\n"]}],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from transformers import ViTModel\n","import os\n","import sys\n","import time\n","import json\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from tqdm.auto import tqdm\n","from torch.utils.data import DataLoader\n","from torch.cuda.amp import autocast, GradScaler\n","from torch.optim import AdamW\n","from torch.optim.lr_scheduler import OneCycleLR, CosineAnnealingWarmRestarts\n","from sklearn.preprocessing import LabelEncoder\n","import cv2\n","from PIL import Image\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","\n","# ==================== CONFIGURATION ====================\n","class TrainingConfig:\n","    \"\"\"Training configuration with all hyperparameters\"\"\"\n","    def __init__(self):\n","        # Paths\n","        self.preprocessed_dir = '/kaggle/working/illinois_doc_preprocessed'\n","        self.output_dir = '/kaggle/working/vit_bmi_checkpoints'\n","        self.log_dir = '/kaggle/working/logs'\n","        \n","        # Training\n","        self.n_folds = 5\n","        self.epochs = 20\n","        self.batch_size = 96\n","        self.accumulation_steps = 1\n","        \n","        # Optimizer\n","        self.lr = 3e-5\n","        self.vit_lr_multiplier = 0.1  # Lower LR for pretrained ViT\n","        self.weight_decay = 0.01\n","        self.max_grad_norm = 1.0\n","        \n","        # Scheduler\n","        self.warmup_pct = 0.1\n","        self.scheduler_type = 'onecycle'\n","        \n","        # Model architecture\n","        self.embed_dim = 32\n","        self.fusion_dim = 512\n","        self.unfreeze_last_n_layers = 4\n","        self.use_adaptive_pooling = False  # Set to True to use complex pooling\n","        \n","        # Loss\n","        self.loss_type = 'huber'  # 'mse', 'mae', 'huber', 'smooth_l1'\n","        self.huber_delta = 1.0\n","        \n","        # Regularization\n","        self.dropout = 0.2\n","        \n","        # Early stopping\n","        self.patience = 7\n","        self.min_delta = 0.0001\n","        \n","        # Data loading\n","        self.num_workers = 8\n","        self.pin_memory = True\n","        self.prefetch_factor = 4\n","        \n","        # Mixed precision\n","        self.use_amp = True\n","        \n","        # Validation\n","        self.val_every_n_epochs = 1\n","        \n","        # Grad-CAM visualization\n","        self.gradcam_frequency = 999  # Generate Grad-CAM every N epochs\n","        self.gradcam_samples = 4\n","        \n","        # Seed\n","        self.seed = 42\n","\n","\n","# ==================== GRAD-CAM FOR VIT (FIXED) ====================\n","class ViTGradCAM:\n","    \"\"\"Improved Grad-CAM for Vision Transformer\"\"\"\n","    def __init__(self, model):\n","        self.model = model\n","        self.gradients = None\n","        self.activations = None\n","        self.handles = []\n","        \n","        # Target the last transformer layer\n","        self.target_layer = model.vit.encoder.layer[-1].output\n","        self._register_hooks()\n","    \n","    def _register_hooks(self):\n","        \"\"\"Register forward and backward hooks\"\"\"\n","        def forward_hook(module, input, output):\n","            # output is a tuple (hidden_states,) for ViT\n","            if isinstance(output, tuple):\n","                self.activations = output[0].detach()\n","            else:\n","                self.activations = output.detach()\n","        \n","        def backward_hook(module, grad_input, grad_output):\n","            # grad_output is a tuple\n","            self.gradients = grad_output[0].detach()\n","        \n","        handle_f = self.target_layer.register_forward_hook(forward_hook)\n","        handle_b = self.target_layer.register_full_backward_hook(backward_hook)\n","        self.handles = [handle_f, handle_b]\n","    \n","    def remove_hooks(self):\n","        \"\"\"Remove all hooks\"\"\"\n","        for handle in self.handles:\n","            handle.remove()\n","    \n","    def generate_cam(self, input_image, categorical_features=None, numerical_features=None):\n","        \"\"\"Generate Class Activation Map\"\"\"\n","        self.model.eval()\n","        \n","        # Get device from model parameters\n","        device = next(self.model.parameters()).device\n","        input_image = input_image.to(device)\n","        \n","        if categorical_features is not None:\n","            categorical_features = categorical_features.to(device)\n","        if numerical_features is not None:\n","            numerical_features = numerical_features.to(device)\n","        \n","        # Enable gradient computation\n","        with torch.enable_grad():\n","            input_image = input_image.requires_grad_(True)\n","            \n","            # Forward pass\n","            output = self.model(input_image, categorical_features, numerical_features)\n","            \n","            # Backward pass\n","            self.model.zero_grad()\n","            output.sum().backward()\n","        \n","        # Get gradients and activations from hooks\n","        gradients = self.gradients  # [B, num_patches+1, hidden_dim]\n","        activations = self.activations  # [B, num_patches+1, hidden_dim]\n","        \n","        # Remove CLS token\n","        gradients = gradients[:, 1:, :]  # [B, num_patches, hidden_dim]\n","        activations = activations[:, 1:, :]  # [B, num_patches, hidden_dim]\n","        \n","        # Global average pooling of gradients\n","        weights = gradients.mean(dim=2, keepdim=True)  # [B, num_patches, 1]\n","        \n","        # Weight the activations\n","        cam = (weights * activations).sum(dim=2)  # [B, num_patches]\n","        \n","        # Apply ReLU\n","        cam = F.relu(cam)\n","        \n","        # Normalize\n","        cam = cam - cam.min()\n","        if cam.max() > 0:\n","            cam = cam / cam.max()\n","        \n","        # Reshape to 2D grid\n","        batch_size = cam.shape[0]\n","        num_patches = cam.shape[1]\n","        grid_size = int(np.sqrt(num_patches))\n","        \n","        if grid_size * grid_size == num_patches:\n","            cam = cam.reshape(batch_size, grid_size, grid_size)\n","        else:\n","            # Fallback for non-square patches\n","            cam = cam.reshape(batch_size, 14, 14)\n","        \n","        # Resize to image size\n","        cam = cam.unsqueeze(1)  # [B, 1, H, W]\n","        cam = F.interpolate(cam, size=(224, 224), mode='bilinear', align_corners=False)\n","        cam = cam.squeeze(1)  # [B, H, W]\n","        \n","        return cam[0].cpu().numpy()  # Return first image\n","    \n","    def visualize_cam(self, input_image, cam):\n","        \"\"\"Create visualization overlaying CAM on original image\"\"\"\n","        # Denormalize image\n","        mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n","        std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n","        \n","        img = input_image.cpu().squeeze() * std + mean\n","        img = torch.clamp(img, 0, 1)\n","        img = img.permute(1, 2, 0).numpy()\n","        img = (img * 255).astype(np.uint8)\n","        \n","        # Create heatmap\n","        heatmap = cv2.applyColorMap(np.uint8(255 * cam), cv2.COLORMAP_JET)\n","        heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)\n","        \n","        # Overlay\n","        result = heatmap * 0.4 + img * 0.6\n","        result = np.clip(result, 0, 255).astype(np.uint8)\n","        \n","        return img, heatmap, result\n","\n","\n","# ==================== ADAPTIVE POOLING (SIMPLIFIED) ====================\n","class SimplifiedAdaptivePooling(nn.Module):\n","    \"\"\"Simplified adaptive pooling with attention weights\"\"\"\n","    def __init__(self, hidden_dim=768):\n","        super(SimplifiedAdaptivePooling, self).__init__()\n","        \n","        # Simple attention scorer\n","        self.attention_scorer = nn.Sequential(\n","            nn.Linear(hidden_dim, hidden_dim // 2),\n","            nn.Tanh(),\n","            nn.Dropout(0.1),\n","            nn.Linear(hidden_dim // 2, 1)\n","        )\n","    \n","    def forward(self, patch_embeddings):\n","        \"\"\"\n","        Args:\n","            patch_embeddings: [batch_size, num_patches, hidden_dim]\n","        Returns:\n","            pooled_output: [batch_size, hidden_dim]\n","            attention_weights: [batch_size, num_patches]\n","        \"\"\"\n","        # Compute attention scores\n","        attention_scores = self.attention_scorer(patch_embeddings).squeeze(-1)  # [B, num_patches]\n","        attention_weights = F.softmax(attention_scores, dim=-1)  # [B, num_patches]\n","        \n","        # Weighted pooling\n","        pooled_output = torch.sum(patch_embeddings * attention_weights.unsqueeze(-1), dim=1)\n","        \n","        return pooled_output, attention_weights\n","\n","\n","# ==================== FEATURE FUSION (SIMPLIFIED) ====================\n","class SimplifiedFeatureFusion(nn.Module):\n","    \"\"\"Simplified feature fusion for multi-modal inputs\"\"\"\n","    def __init__(self, visual_dim, categorical_dim, numerical_dim, fusion_dim=512):\n","        super(SimplifiedFeatureFusion, self).__init__()\n","        \n","        self.visual_dim = visual_dim\n","        self.categorical_dim = categorical_dim\n","        self.numerical_dim = numerical_dim\n","        \n","        # Calculate total input dimension\n","        total_dim = visual_dim\n","        if categorical_dim > 0:\n","            total_dim += categorical_dim\n","        if numerical_dim > 0:\n","            total_dim += numerical_dim\n","        \n","        # Simple fusion network\n","        self.fusion = nn.Sequential(\n","            nn.Linear(total_dim, fusion_dim * 2),\n","            nn.LayerNorm(fusion_dim * 2),\n","            nn.GELU(),\n","            nn.Dropout(0.15),\n","            nn.Linear(fusion_dim * 2, fusion_dim),\n","            nn.LayerNorm(fusion_dim),\n","            nn.GELU(),\n","            nn.Dropout(0.15)\n","        )\n","    \n","    def forward(self, visual_features, categorical_features=None, numerical_features=None):\n","        \"\"\"\n","        Args:\n","            visual_features: [batch_size, visual_dim]\n","            categorical_features: [batch_size, categorical_dim] or None\n","            numerical_features: [batch_size, numerical_dim] or None\n","        \"\"\"\n","        features_list = [visual_features]\n","        \n","        if categorical_features is not None and self.categorical_dim > 0:\n","            features_list.append(categorical_features)\n","        \n","        if numerical_features is not None and self.numerical_dim > 0:\n","            features_list.append(numerical_features)\n","        \n","        # Concatenate all features\n","        fused_features = torch.cat(features_list, dim=-1)\n","        \n","        # Apply fusion network\n","        output = self.fusion(fused_features)\n","        \n","        return output\n","\n","\n","# ==================== REGRESSION HEAD ====================\n","class RegressionHead(nn.Module):\n","    \"\"\"Enhanced regression head with residual connections\"\"\"\n","    def __init__(self, input_dim=512, hidden_dims=[256, 128], dropout=0.2):\n","        super(RegressionHead, self).__init__()\n","        \n","        layers = []\n","        prev_dim = input_dim\n","        \n","        for hidden_dim in hidden_dims:\n","            layers.extend([\n","                nn.Linear(prev_dim, hidden_dim),\n","                nn.LayerNorm(hidden_dim),\n","                nn.GELU(),\n","                nn.Dropout(dropout)\n","            ])\n","            prev_dim = hidden_dim\n","        \n","        self.layers = nn.Sequential(*layers)\n","        \n","        # Final prediction layer\n","        self.output = nn.Linear(prev_dim, 1)\n","        \n","        # Residual connection if dimensions allow\n","        self.residual = nn.Linear(input_dim, hidden_dims[-1]) if input_dim != hidden_dims[-1] else nn.Identity()\n","    \n","    def forward(self, x):\n","        residual = self.residual(x)\n","        out = self.layers(x)\n","        \n","        # Add residual before final prediction\n","        if out.shape[-1] == residual.shape[-1]:\n","            out = out + residual\n","        \n","        return self.output(out)\n","\n","\n","# ==================== MAIN MODEL (FIXED) ====================\n","class ViTForBMI(nn.Module):\n","    \"\"\"Vision Transformer for BMI Prediction with Multi-Modal Features\"\"\"\n","    \n","    def __init__(self, num_categorical_features=0, categorical_vocab_sizes=None, \n","                 num_numerical_features=0, embed_dim=32, fusion_dim=512,\n","                 freeze_backbone=True, unfreeze_last_n_layers=4,\n","                 use_adaptive_pooling=False, dropout=0.2):\n","        super(ViTForBMI, self).__init__()\n","        \n","        # Load pretrained ViT\n","        self.vit = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\n","        \n","        # Freeze backbone if specified\n","        if freeze_backbone:\n","            for param in self.vit.parameters():\n","                param.requires_grad = False\n","            \n","            # Unfreeze last N transformer layers\n","            for layer in self.vit.encoder.layer[-unfreeze_last_n_layers:]:\n","                for param in layer.parameters():\n","                    param.requires_grad = True\n","        \n","        # Pooling strategy\n","        self.use_adaptive_pooling = use_adaptive_pooling\n","        if use_adaptive_pooling:\n","            self.adaptive_pooling = SimplifiedAdaptivePooling(hidden_dim=768)\n","        \n","        # Categorical embeddings (FIXED)\n","        self.categorical_embeddings = None\n","        categorical_dim = 0\n","        if num_categorical_features > 0 and categorical_vocab_sizes:\n","            self.categorical_embeddings = nn.ModuleList([\n","                nn.Embedding(vocab_size + 1, embed_dim, padding_idx=0)  # +1 for padding\n","                for vocab_size in categorical_vocab_sizes\n","            ])\n","            categorical_dim = num_categorical_features * embed_dim\n","        \n","        # Numerical feature projection\n","        self.numerical_proj = None\n","        numerical_dim = 0\n","        if num_numerical_features > 0:\n","            numerical_dim = 128\n","            self.numerical_proj = nn.Sequential(\n","                nn.Linear(num_numerical_features, 256),\n","                nn.LayerNorm(256),\n","                nn.GELU(),\n","                nn.Dropout(dropout),\n","                nn.Linear(256, numerical_dim),\n","                nn.LayerNorm(numerical_dim)\n","            )\n","        \n","        # Feature fusion (simplified)\n","        self.feature_fusion = SimplifiedFeatureFusion(\n","            visual_dim=768,\n","            categorical_dim=categorical_dim,\n","            numerical_dim=numerical_dim,\n","            fusion_dim=fusion_dim\n","        )\n","        \n","        # Regression head\n","        self.regression_head = RegressionHead(\n","            input_dim=fusion_dim,\n","            hidden_dims=[256, 128],\n","            dropout=dropout\n","        )\n","        \n","        # Initialize weights\n","        self._initialize_weights()\n","        \n","        print(f\"\\n{'='*70}\")\n","        print(\"MODEL ARCHITECTURE - ViT for BMI Prediction\")\n","        print(f\"{'='*70}\")\n","        print(f\"Visual Feature Dim:      768 (ViT-Base)\")\n","        print(f\"Pooling Strategy:        {'Adaptive' if use_adaptive_pooling else 'CLS Token'}\")\n","        print(f\"Categorical Features:    {num_categorical_features} -> {categorical_dim}\")\n","        print(f\"Numerical Features:      {num_numerical_features} -> {numerical_dim}\")\n","        print(f\"Fusion Dim:              {fusion_dim}\")\n","        print(f\"Regression Head:         {fusion_dim} -> 256 -> 128 -> 1\")\n","        print(f\"Unfrozen ViT Layers:     Last {unfreeze_last_n_layers}\")\n","        print(f\"Dropout:                 {dropout}\")\n","        print(f\"{'='*70}\\n\")\n","    \n","    def _initialize_weights(self):\n","        \"\"\"Initialize custom layer weights\"\"\"\n","        modules = [self.feature_fusion, self.regression_head]\n","        if self.use_adaptive_pooling:\n","            modules.append(self.adaptive_pooling)\n","        if self.numerical_proj is not None:\n","            modules.append(self.numerical_proj)\n","        \n","        for module in modules:\n","            for m in module.modules():\n","                if isinstance(m, nn.Linear):\n","                    nn.init.xavier_uniform_(m.weight)\n","                    if m.bias is not None:\n","                        nn.init.constant_(m.bias, 0)\n","                elif isinstance(m, nn.LayerNorm):\n","                    nn.init.constant_(m.weight, 1)\n","                    nn.init.constant_(m.bias, 0)\n","    \n","    def forward(self, pixel_values, categorical_features=None, numerical_features=None):\n","        \"\"\"\n","        Args:\n","            pixel_values: [batch_size, 3, 224, 224]\n","            categorical_features: [batch_size, num_categorical] or None\n","            numerical_features: [batch_size, num_numerical] or None\n","        Returns:\n","            bmi_prediction: [batch_size, 1]\n","        \"\"\"\n","        # Extract ViT features\n","        outputs = self.vit(pixel_values=pixel_values)\n","        last_hidden_state = outputs.last_hidden_state  # [B, num_patches+1, 768]\n","        \n","        # Get visual features\n","        if self.use_adaptive_pooling:\n","            # Remove CLS token, use adaptive pooling on patch tokens\n","            patch_tokens = last_hidden_state[:, 1:, :]  # [B, num_patches, 768]\n","            visual_features, _ = self.adaptive_pooling(patch_tokens)  # [B, 768]\n","        else:\n","            # Use CLS token (simpler and often works better)\n","            visual_features = last_hidden_state[:, 0, :]  # [B, 768]\n","        \n","        # Process categorical features (FIXED)\n","        cat_features = None\n","        if categorical_features is not None and self.categorical_embeddings is not None:\n","            cat_embeds = []\n","            for i, embedding_layer in enumerate(self.categorical_embeddings):\n","                feat = categorical_features[:, i]\n","                \n","                # Handle negative values (unknown) -> map to 0 (padding)\n","                feat = torch.where(feat < 0, torch.zeros_like(feat), feat + 1)\n","                \n","                # Clamp to valid range\n","                max_idx = embedding_layer.num_embeddings - 1\n","                feat = torch.clamp(feat, min=0, max=max_idx)\n","                \n","                emb = embedding_layer(feat)\n","                cat_embeds.append(emb)\n","            \n","            cat_features = torch.cat(cat_embeds, dim=-1)  # [B, categorical_dim]\n","        \n","        # Process numerical features\n","        num_features = None\n","        if numerical_features is not None and self.numerical_proj is not None:\n","            # Handle NaN and inf values\n","            numerical_features = torch.nan_to_num(numerical_features, nan=0.0, posinf=0.0, neginf=0.0)\n","            num_features = self.numerical_proj(numerical_features)  # [B, numerical_dim]\n","        \n","        # Feature fusion\n","        fused_features = self.feature_fusion(visual_features, cat_features, num_features)\n","        \n","        # Regression\n","        bmi_prediction = self.regression_head(fused_features)\n","        \n","        return bmi_prediction\n","    \n","    def visualize_predictions_with_gradcam(self, batch, gradcam, save_dir, num_samples=8):\n","        \"\"\"Generate Grad-CAM visualizations (FIXED)\"\"\"\n","        self.eval()\n","        os.makedirs(save_dir, exist_ok=True)\n","        \n","        # Get device from model parameters\n","        device = next(self.parameters()).device\n","        \n","        images = batch['image'][:num_samples]\n","        targets = batch['bmi'][:num_samples]\n","        image_names = batch['image_name'][:num_samples]\n","        \n","        categorical_features = batch.get('categorical_features')\n","        if categorical_features is not None:\n","            categorical_features = categorical_features[:num_samples]\n","        \n","        numerical_features = batch.get('numerical_features')\n","        if numerical_features is not None:\n","            numerical_features = numerical_features[:num_samples]\n","        \n","        # Get predictions\n","        with torch.no_grad():\n","            predictions = self(\n","                images.to(device), \n","                categorical_features.to(device) if categorical_features is not None else None,\n","                numerical_features.to(device) if numerical_features is not None else None\n","            )\n","        \n","        # Create figure\n","        fig, axes = plt.subplots(num_samples, 4, figsize=(16, 4*num_samples))\n","        if num_samples == 1:\n","            axes = axes.reshape(1, -1)\n","        \n","        for idx in range(num_samples):\n","            img_tensor = images[idx:idx+1]\n","            cat_feat = categorical_features[idx:idx+1] if categorical_features is not None else None\n","            num_feat = numerical_features[idx:idx+1] if numerical_features is not None else None\n","            \n","            # Generate Grad-CAM\n","            try:\n","                cam = gradcam.generate_cam(img_tensor, cat_feat, num_feat)\n","                original, heatmap, overlay = gradcam.visualize_cam(img_tensor, cam)\n","            except Exception as e:\n","                print(f\"Warning: Grad-CAM failed for sample {idx}: {e}\")\n","                # Use dummy images\n","                original = np.zeros((224, 224, 3), dtype=np.uint8)\n","                heatmap = np.zeros((224, 224, 3), dtype=np.uint8)\n","                overlay = np.zeros((224, 224, 3), dtype=np.uint8)\n","            \n","            pred_bmi = predictions[idx].item()\n","            true_bmi = targets[idx].item()\n","            error = abs(pred_bmi - true_bmi)\n","            \n","            # Original image\n","            axes[idx, 0].imshow(original)\n","            axes[idx, 0].set_title(f'Original\\n{image_names[idx][:20]}', fontsize=10)\n","            axes[idx, 0].axis('off')\n","            \n","            # Heatmap\n","            axes[idx, 1].imshow(heatmap)\n","            axes[idx, 1].set_title(f'Attention Heatmap', fontsize=10)\n","            axes[idx, 1].axis('off')\n","            \n","            # Overlay\n","            axes[idx, 2].imshow(overlay)\n","            axes[idx, 2].set_title(f'Overlay', fontsize=10)\n","            axes[idx, 2].axis('off')\n","            \n","            # Prediction comparison\n","            axes[idx, 3].bar(['True', 'Pred'], [true_bmi, pred_bmi], \n","                           color=['steelblue', 'coral'], alpha=0.7, edgecolor='black')\n","            axes[idx, 3].set_ylabel('BMI', fontsize=10)\n","            axes[idx, 3].set_title(f'True: {true_bmi:.2f} | Pred: {pred_bmi:.2f}\\nError: {error:.2f}', \n","                                  fontsize=10, fontweight='bold')\n","            axes[idx, 3].grid(True, alpha=0.3, axis='y')\n","            axes[idx, 3].set_ylim(15, 50)\n","        \n","        plt.tight_layout()\n","        plt.savefig(f'{save_dir}/gradcam_visualization.png', dpi=150, bbox_inches='tight')\n","        plt.close()\n","        \n","        print(f\"  ✓ Grad-CAM saved to {save_dir}/gradcam_visualization.png\")\n","\n","\n","def count_parameters(model):\n","    \"\"\"Count trainable and total parameters\"\"\"\n","    total_params = sum(p.numel() for p in model.parameters())\n","    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","    frozen_params = total_params - trainable_params\n","    \n","    print(f\"\\nModel Parameter Summary:\")\n","    print(f\"{'='*50}\")\n","    print(f\"Total Parameters:      {total_params:,}\")\n","    print(f\"Trainable Parameters:  {trainable_params:,} ({trainable_params/total_params*100:.2f}%)\")\n","    print(f\"Frozen Parameters:     {frozen_params:,} ({frozen_params/total_params*100:.2f}%)\")\n","    print(f\"{'='*50}\\n\")\n","    \n","    return total_params, trainable_params\n","\n","\n","# ==================== DATASET (FIXED) ====================\n","class BMIDataset(torch.utils.data.Dataset):\n","    \"\"\"Dataset for BMI prediction with proper feature detection\"\"\"\n","    def __init__(self, dataframe, transform=None):\n","        self.df = dataframe.reset_index(drop=True)\n","        self.transform = transform\n","        \n","        # Identify categorical features (encoded columns)\n","        self.categorical_cols = [col for col in self.df.columns if col.endswith('_encoded')]\n","        \n","        # Identify numerical features (FIXED to match preprocessing)\n","        self.numerical_cols = []\n","        for col in self.df.columns:\n","            # Skip non-feature columns\n","            if col in ['name', 'image_path', 'bmi', 'image_stem', 'has_image']:\n","                continue\n","            \n","            # Check if it's a numerical feature we want\n","            if self.df[col].dtype in [np.float32, np.float64, np.int32, np.int64]:\n","                # Include features that end with specific suffixes\n","                if any(col.endswith(suffix) for suffix in [\n","                    '_zscore', '_percentile', '_squared', '_deviation', \n","                    '_above_mean', '_product', '_interaction'\n","                ]):\n","                    self.numerical_cols.append(col)\n","                # Include specific named features\n","                elif col in ['bsa', 'ponderal_index', 'height_m', 'weight_kg', \n","                           'age', 'age_decade', 'height', 'weight']:\n","                    self.numerical_cols.append(col)\n","        \n","        print(f\"  Dataset features: {len(self.categorical_cols)} categorical, {len(self.numerical_cols)} numerical\")\n","    \n","    def __len__(self):\n","        return len(self.df)\n","    \n","    def __getitem__(self, idx):\n","        row = self.df.iloc[idx]\n","        \n","        # Load image\n","        img_path = row['image_path']\n","        try:\n","            with Image.open(img_path) as image:\n","                image = image.convert('RGB')\n","                if self.transform:\n","                    image = self.transform(image)\n","        except Exception as e:\n","            # Return zero tensor if image fails\n","            print(f\"Warning: Failed to load image {img_path}: {e}\")\n","            image = torch.zeros(3, 224, 224)\n","        \n","        # Get BMI target\n","        bmi = torch.tensor(row['bmi'], dtype=torch.float32)\n","        \n","        # Get categorical features\n","        categorical_features = None\n","        if len(self.categorical_cols) > 0:\n","            categorical_features = torch.tensor(\n","                [row[col] for col in self.categorical_cols], \n","                dtype=torch.long\n","            )\n","        \n","        # Get numerical features\n","        numerical_features = None\n","        if len(self.numerical_cols) > 0:\n","            numerical_features = torch.tensor(\n","                [row[col] for col in self.numerical_cols], \n","                dtype=torch.float32\n","            )\n","        \n","        return {\n","            'image': image,\n","            'bmi': bmi,\n","            'categorical_features': categorical_features,\n","            'numerical_features': numerical_features,\n","            'image_name': row['name']\n","        }\n","\n","\n","# ==================== DATA LOADING UTILITIES ====================\n","def get_transforms(config_dict):\n","    \"\"\"Create data transforms with conservative augmentation for faces\"\"\"\n","    from torchvision import transforms\n","    \n","    # Conservative augmentation for face images\n","    train_transform = transforms.Compose([\n","        transforms.Resize((224, 224)),\n","        transforms.RandomHorizontalFlip(p=0.3),  # Reduced from 0.5\n","        transforms.RandomRotation(degrees=5),     # Reduced from 10\n","        transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.05),  # Reduced\n","        transforms.RandomAffine(degrees=0, translate=(0.05, 0.05)),  # Small shifts\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean=config_dict['mean'], std=config_dict['std'])\n","    ])\n","    \n","    val_transform = transforms.Compose([\n","        transforms.Resize((224, 224)),\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean=config_dict['mean'], std=config_dict['std'])\n","    ])\n","    \n","    return train_transform, val_transform\n","\n","\n","def load_config_and_encoders(preprocessed_dir):\n","    \"\"\"Load preprocessing config and feature encoders\"\"\"\n","    with open(f'{preprocessed_dir}/config.json', 'r') as f:\n","        config_dict = json.load(f)\n","    \n","    with open(f'{preprocessed_dir}/feature_encoders.json', 'r') as f:\n","        encoder_dict = json.load(f)\n","    \n","    # Reconstruct encoders\n","    feature_encoders = {}\n","    for feature, info in encoder_dict.items():\n","        le = LabelEncoder()\n","        le.classes_ = np.array(info['classes'])\n","        feature_encoders[feature] = le\n","    \n","    return config_dict, feature_encoders\n","\n","\n","def get_categorical_vocab_sizes(feature_encoders):\n","    \"\"\"Get vocabulary sizes for categorical features (FIXED)\"\"\"\n","    # Feature encoders are stored with original names ('race', 'sex', etc.)\n","    # Return in sorted order for consistency\n","    vocab_sizes = []\n","    for feat_name in sorted(feature_encoders.keys()):\n","        vocab_sizes.append(len(feature_encoders[feat_name].classes_))\n","    return vocab_sizes\n","\n","\n","def validate_fold_features(train_df, val_df):\n","    \"\"\"Validate that train and val have same features\"\"\"\n","    train_features = set(train_df.columns)\n","    val_features = set(val_df.columns)\n","    \n","    if train_features != val_features:\n","        missing_in_val = train_features - val_features\n","        missing_in_train = val_features - train_features\n","        \n","        error_msg = \"Feature mismatch between train and validation!\\n\"\n","        if missing_in_val:\n","            error_msg += f\"Missing in val: {missing_in_val}\\n\"\n","        if missing_in_train:\n","            error_msg += f\"Missing in train: {missing_in_train}\\n\"\n","        \n","        raise ValueError(error_msg)\n","    \n","    print(f\"  ✓ Feature validation passed: {len(train_features)} features\")\n","\n","\n","def get_dataloaders(fold, config, preprocess_config):\n","    \"\"\"Create train and validation dataloaders with validation\"\"\"\n","    train_transform, val_transform = get_transforms(preprocess_config)\n","    \n","    # Load data\n","    train_df = pd.read_csv(f'{config.preprocessed_dir}/fold_{fold}/train.csv')\n","    val_df = pd.read_csv(f'{config.preprocessed_dir}/fold_{fold}/val.csv')\n","    \n","    print(f\"\\nFold {fold} dataset sizes:\")\n","    print(f\"  Train: {len(train_df):,} samples\")\n","    print(f\"  Val:   {len(val_df):,} samples\")\n","    \n","    # Validate features match\n","    validate_fold_features(train_df, val_df)\n","    \n","    # Create datasets\n","    train_dataset = BMIDataset(train_df, transform=train_transform)\n","    val_dataset = BMIDataset(val_df, transform=val_transform)\n","    \n","    # Create dataloaders\n","    train_loader = DataLoader(\n","        train_dataset,\n","        batch_size=config.batch_size,\n","        shuffle=True,\n","        num_workers=config.num_workers,\n","        pin_memory=config.pin_memory,\n","        prefetch_factor=config.prefetch_factor,\n","        persistent_workers=True if config.num_workers > 0 else False,\n","        drop_last=True  # Drop last incomplete batch for stable training\n","    )\n","    \n","    val_loader = DataLoader(\n","        val_dataset,\n","        batch_size=config.batch_size * 2,\n","        shuffle=False,\n","        num_workers=config.num_workers,\n","        pin_memory=config.pin_memory,\n","        prefetch_factor=config.prefetch_factor,\n","        persistent_workers=True if config.num_workers > 0 else False\n","    )\n","    \n","    return train_loader, val_loader\n","\n","\n","# ==================== LOSS FUNCTIONS ====================\n","def get_loss_function(config):\n","    \"\"\"Get loss function based on config\"\"\"\n","    if config.loss_type == 'mse':\n","        return nn.MSELoss()\n","    elif config.loss_type == 'mae':\n","        return nn.L1Loss()\n","    elif config.loss_type == 'huber':\n","        return nn.HuberLoss(delta=config.huber_delta)\n","    elif config.loss_type == 'smooth_l1':\n","        return nn.SmoothL1Loss()\n","    else:\n","        raise ValueError(f\"Unknown loss type: {config.loss_type}\")\n","\n","\n","# ==================== METRICS ====================\n","def calculate_metrics(predictions, targets):\n","    \"\"\"Calculate regression metrics\"\"\"\n","    predictions = np.array(predictions).flatten()\n","    targets = np.array(targets).flatten()\n","    \n","    mae = np.mean(np.abs(predictions - targets))\n","    mse = np.mean((predictions - targets) ** 2)\n","    rmse = np.sqrt(mse)\n","    \n","    # R² score\n","    ss_res = np.sum((targets - predictions) ** 2)\n","    ss_tot = np.sum((targets - np.mean(targets)) ** 2)\n","    r2 = 1 - (ss_res / ss_tot) if ss_tot != 0 else 0\n","    \n","    return {\n","        'mae': mae,\n","        'mse': mse,\n","        'rmse': rmse,\n","        'r2': r2\n","    }\n","\n","\n","# ==================== UTILITIES ====================\n","def set_seed(seed=42):\n","    \"\"\"Set random seeds for reproducibility\"\"\"\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","    np.random.seed(seed)\n","    \n","    if torch.cuda.is_available():\n","        torch.backends.cudnn.deterministic = True\n","        torch.backends.cudnn.benchmark = False\n","\n","\n","def get_device():\n","    \"\"\"Get available device\"\"\"\n","    if torch.cuda.is_available():\n","        device = torch.device('cuda')\n","        print(f\"✓ Using GPU: {torch.cuda.get_device_name(0)}\")\n","        print(f\"  Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n","    else:\n","        device = torch.device('cpu')\n","        print(\"⚠ Using CPU\")\n","    return device\n","\n","\n","# ==================== EARLY STOPPING ====================\n","class EarlyStopping:\n","    \"\"\"Early stopping handler\"\"\"\n","    def __init__(self, patience=7, min_delta=0.0001, mode='min'):\n","        self.patience = patience\n","        self.min_delta = min_delta\n","        self.mode = mode\n","        self.counter = 0\n","        self.best_score = None\n","        self.early_stop = False\n","        self.best_epoch = 0\n","    \n","    def __call__(self, score, epoch):\n","        if self.best_score is None:\n","            self.best_score = score\n","            self.best_epoch = epoch\n","            return False\n","        \n","        if self.mode == 'min':\n","            improved = score < (self.best_score - self.min_delta)\n","        else:\n","            improved = score > (self.best_score + self.min_delta)\n","        \n","        if improved:\n","            self.best_score = score\n","            self.best_epoch = epoch\n","            self.counter = 0\n","            return False\n","        else:\n","            self.counter += 1\n","            if self.counter >= self.patience:\n","                self.early_stop = True\n","            return False\n","\n","\n","# ==================== TRAINING ====================\n","def train_epoch(model, train_loader, criterion, optimizer, scheduler, scaler, \n","                device, config, epoch):\n","    \"\"\"Train for one epoch (FIXED gradient accumulation)\"\"\"\n","    model.train()\n","    \n","    total_loss = 0\n","    num_batches = len(train_loader)\n","    \n","    pbar = tqdm(train_loader, desc=f'Epoch {epoch}', leave=False)\n","    \n","    optimizer.zero_grad()\n","    \n","    for batch_idx, batch in enumerate(pbar):\n","        # Move to device\n","        images = batch['image'].to(device, non_blocking=True)\n","        targets = batch['bmi'].to(device, non_blocking=True)\n","        \n","        categorical_features = batch.get('categorical_features')\n","        if categorical_features is not None:\n","            categorical_features = categorical_features.to(device, non_blocking=True)\n","        \n","        numerical_features = batch.get('numerical_features')\n","        if numerical_features is not None:\n","            numerical_features = numerical_features.to(device, non_blocking=True)\n","        \n","        # Forward pass with mixed precision\n","        with autocast(enabled=config.use_amp):\n","            predictions = model(images, categorical_features, numerical_features)\n","            loss = criterion(predictions.squeeze(), targets)\n","            \n","            # Scale loss for gradient accumulation\n","            loss = loss / config.accumulation_steps\n","        \n","        # Backward pass\n","        scaler.scale(loss).backward()\n","        \n","        # Update weights every accumulation_steps OR at the last batch (FIXED)\n","        is_accumulation_step = (batch_idx + 1) % config.accumulation_steps == 0\n","        is_last_batch = (batch_idx + 1) == num_batches\n","        \n","        if is_accumulation_step or is_last_batch:\n","            # Gradient clipping\n","            scaler.unscale_(optimizer)\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), config.max_grad_norm)\n","            \n","            # Optimizer step\n","            scaler.step(optimizer)\n","            scaler.update()\n","            \n","            # Scheduler step\n","            if scheduler is not None:\n","                scheduler.step()\n","            \n","            optimizer.zero_grad()\n","        \n","        # Track loss\n","        total_loss += loss.item() * config.accumulation_steps\n","        \n","        # Update progress bar\n","        pbar.set_postfix({'loss': f'{total_loss / (batch_idx + 1):.4f}'})\n","    \n","    return total_loss / num_batches\n","\n","\n","@torch.no_grad()\n","def validate(model, val_loader, criterion, device, config):\n","    \"\"\"Validate model\"\"\"\n","    model.eval()\n","    \n","    total_loss = 0\n","    all_predictions = []\n","    all_targets = []\n","    \n","    pbar = tqdm(val_loader, desc='Validating', leave=False)\n","    \n","    for batch in pbar:\n","        images = batch['image'].to(device, non_blocking=True)\n","        targets = batch['bmi'].to(device, non_blocking=True)\n","        \n","        categorical_features = batch.get('categorical_features')\n","        if categorical_features is not None:\n","            categorical_features = categorical_features.to(device, non_blocking=True)\n","        \n","        numerical_features = batch.get('numerical_features')\n","        if numerical_features is not None:\n","            numerical_features = numerical_features.to(device, non_blocking=True)\n","        \n","        # Forward pass\n","        with autocast(enabled=config.use_amp):\n","            predictions = model(images, categorical_features, numerical_features)\n","            loss = criterion(predictions.squeeze(), targets)\n","        \n","        total_loss += loss.item()\n","        \n","        # Collect predictions and targets\n","        all_predictions.extend(predictions.cpu().numpy().flatten())\n","        all_targets.extend(targets.cpu().numpy().flatten())\n","    \n","    avg_loss = total_loss / len(val_loader)\n","    metrics = calculate_metrics(all_predictions, all_targets)\n","    metrics['loss'] = avg_loss\n","    \n","    return metrics\n","\n","\n","# ==================== CHECKPOINTING ====================\n","def save_checkpoint(model, optimizer, scheduler, epoch, metrics, fold, config):\n","    \"\"\"Save model checkpoint\"\"\"\n","    os.makedirs(config.output_dir, exist_ok=True)\n","    \n","    checkpoint = {\n","        'epoch': epoch,\n","        'model_state_dict': model.state_dict(),\n","        'optimizer_state_dict': optimizer.state_dict(),\n","        'scheduler_state_dict': scheduler.state_dict() if scheduler else None,\n","        'metrics': metrics,\n","        'config': config.__dict__\n","    }\n","    \n","    path = f'{config.output_dir}/fold_{fold}_best.pt'\n","    torch.save(checkpoint, path)\n","    \n","    return path\n","\n","\n","def load_checkpoint(path, model, optimizer=None, scheduler=None):\n","    \"\"\"Load model checkpoint\"\"\"\n","    checkpoint = torch.load(path, map_location='cpu', weights_only=False)\n","    \n","    model.load_state_dict(checkpoint['model_state_dict'])\n","    \n","    if optimizer and 'optimizer_state_dict' in checkpoint:\n","        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","    \n","    if scheduler and 'scheduler_state_dict' in checkpoint and checkpoint['scheduler_state_dict']:\n","        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n","    \n","    return checkpoint['epoch'], checkpoint['metrics']\n","\n","\n","# ==================== TRAINING LOOP ====================\n","def train_fold(fold, config, device):\n","    \"\"\"Train model for one fold\"\"\"\n","    print(f\"\\n{'='*70}\")\n","    print(f\"TRAINING FOLD {fold}/{config.n_folds}\")\n","    print(f\"{'='*70}\\n\")\n","    \n","    # Load preprocessing config and encoders\n","    preprocess_config, feature_encoders = load_config_and_encoders(config.preprocessed_dir)\n","    \n","    # Get dataloaders\n","    train_loader, val_loader = get_dataloaders(fold, config, preprocess_config)\n","    \n","    # Get feature dimensions from first batch\n","    sample_batch = next(iter(train_loader))\n","    num_categorical = sample_batch['categorical_features'].shape[1] if sample_batch['categorical_features'] is not None else 0\n","    num_numerical = sample_batch['numerical_features'].shape[1] if sample_batch['numerical_features'] is not None else 0\n","    categorical_vocab_sizes = get_categorical_vocab_sizes(feature_encoders)\n","    \n","    print(f\"\\nFeature configuration:\")\n","    print(f\"  Categorical features: {num_categorical}\")\n","    print(f\"  Numerical features:   {num_numerical}\")\n","    print(f\"  Vocabulary sizes:     {categorical_vocab_sizes}\\n\")\n","    \n","    # Create model\n","    model = ViTForBMI(\n","        num_categorical_features=num_categorical,\n","        categorical_vocab_sizes=categorical_vocab_sizes,\n","        num_numerical_features=num_numerical,\n","        embed_dim=config.embed_dim,\n","        fusion_dim=config.fusion_dim,\n","        unfreeze_last_n_layers=config.unfreeze_last_n_layers,\n","        use_adaptive_pooling=config.use_adaptive_pooling,\n","        dropout=config.dropout\n","    ).to(device)\n","    \n","    # Count parameters\n","    count_parameters(model)\n","    \n","    # Initialize Grad-CAM\n","    gradcam = ViTGradCAM(model)\n","    print(\"✓ Grad-CAM initialized\\n\")\n","    \n","    # Create visualization directory\n","    viz_dir = f'{config.output_dir}/gradcam_fold_{fold}'\n","    os.makedirs(viz_dir, exist_ok=True)\n","    \n","    # Get loss function\n","    criterion = get_loss_function(config)\n","    print(f\"Loss function: {config.loss_type}\\n\")\n","    \n","    # Create optimizer with different learning rates for ViT and new layers\n","    vit_params = [p for n, p in model.named_parameters() if 'vit' in n and p.requires_grad]\n","    other_params = [p for n, p in model.named_parameters() if 'vit' not in n and p.requires_grad]\n","    \n","    optimizer = AdamW([\n","        {'params': vit_params, 'lr': config.lr * config.vit_lr_multiplier},\n","        {'params': other_params, 'lr': config.lr}\n","    ], weight_decay=config.weight_decay)\n","    \n","    # Calculate total training steps (FIXED)\n","    steps_per_epoch = len(train_loader) // config.accumulation_steps\n","    if len(train_loader) % config.accumulation_steps != 0:\n","        steps_per_epoch += 1  # Account for partial accumulation\n","    \n","    total_steps = steps_per_epoch * config.epochs\n","    warmup_steps = int(total_steps * config.warmup_pct)\n","    \n","    print(f\"Training schedule:\")\n","    print(f\"  Steps per epoch: {steps_per_epoch}\")\n","    print(f\"  Total steps: {total_steps}\")\n","    print(f\"  Warmup steps: {warmup_steps}\\n\")\n","    \n","    # Create scheduler\n","    if config.scheduler_type == 'onecycle':\n","        scheduler = OneCycleLR(\n","            optimizer,\n","            max_lr=[config.lr * config.vit_lr_multiplier, config.lr],\n","            total_steps=total_steps,\n","            pct_start=config.warmup_pct,\n","            anneal_strategy='cos',\n","            div_factor=25,\n","            final_div_factor=10000\n","        )\n","    elif config.scheduler_type == 'cosine':\n","        scheduler = CosineAnnealingWarmRestarts(\n","            optimizer,\n","            T_0=steps_per_epoch * 5,\n","            T_mult=1\n","        )\n","    else:\n","        scheduler = None\n","    \n","    # Mixed precision scaler\n","    scaler = GradScaler(enabled=config.use_amp)\n","    \n","    # Early stopping\n","    early_stopping = EarlyStopping(patience=config.patience, min_delta=config.min_delta)\n","    \n","    # Training history\n","    history = {\n","        'train_losses': [],\n","        'val_losses': [],\n","        'val_maes': [],\n","        'val_mses': [],\n","        'val_rmses': [],\n","        'val_r2s': []\n","    }\n","    \n","    best_mae = float('inf')\n","    best_metrics = None\n","    \n","    print(f\"Starting training...\")\n","    print(f\"{'='*70}\\n\")\n","    \n","    start_time = time.time()\n","    \n","    for epoch in range(1, config.epochs + 1):\n","        epoch_start = time.time()\n","        \n","        # Train\n","        train_loss = train_epoch(model, train_loader, criterion, optimizer, \n","                                scheduler, scaler, device, config, epoch)\n","        history['train_losses'].append(train_loss)\n","        \n","        # Validate\n","        if epoch % config.val_every_n_epochs == 0 or epoch == config.epochs:\n","            val_metrics = validate(model, val_loader, criterion, device, config)\n","            \n","            history['val_losses'].append(val_metrics['loss'])\n","            history['val_maes'].append(val_metrics['mae'])\n","            history['val_mses'].append(val_metrics['mse'])\n","            history['val_rmses'].append(val_metrics['rmse'])\n","            history['val_r2s'].append(val_metrics['r2'])\n","            \n","            epoch_time = time.time() - epoch_start\n","            \n","            print(f\"Epoch {epoch:02d}/{config.epochs} | \"\n","                  f\"Time: {epoch_time:.1f}s | \"\n","                  f\"Train Loss: {train_loss:.4f} | \"\n","                  f\"Val Loss: {val_metrics['loss']:.4f} | \"\n","                  f\"MAE: {val_metrics['mae']:.4f} | \"\n","                  f\"RMSE: {val_metrics['rmse']:.4f} | \"\n","                  f\"R²: {val_metrics['r2']:.4f}\")\n","            \n","            # Generate Grad-CAM visualizations periodically\n","            if epoch % config.gradcam_frequency == 0 or epoch == 1:\n","                print(\"  Generating Grad-CAM visualizations...\")\n","                try:\n","                    val_batch = next(iter(val_loader))\n","                    model.visualize_predictions_with_gradcam(\n","                        val_batch, gradcam, \n","                        f'{viz_dir}/epoch_{epoch}',\n","                        num_samples=config.gradcam_samples\n","                    )\n","                except Exception as e:\n","                    print(f\"  ⚠ Grad-CAM generation failed: {str(e)}\")\n","            \n","            # Save best model\n","            if val_metrics['mae'] < best_mae:\n","                best_mae = val_metrics['mae']\n","                best_metrics = val_metrics.copy()\n","                checkpoint_path = save_checkpoint(model, optimizer, scheduler, \n","                                                 epoch, val_metrics, fold, config)\n","                print(f\"  ✓ New best model saved (MAE: {best_mae:.4f})\")\n","            \n","            # Early stopping check\n","            early_stopping(val_metrics['mae'], epoch)\n","            if early_stopping.early_stop:\n","                print(f\"\\n  Early stopping triggered at epoch {epoch}\")\n","                print(f\"  Best epoch was {early_stopping.best_epoch}\")\n","                break\n","        else:\n","            epoch_time = time.time() - epoch_start\n","            print(f\"Epoch {epoch:02d}/{config.epochs} | \"\n","                  f\"Time: {epoch_time:.1f}s | \"\n","                  f\"Train Loss: {train_loss:.4f}\")\n","    \n","    total_time = time.time() - start_time\n","    \n","    # Load best model for final evaluation\n","    checkpoint_path = f'{config.output_dir}/fold_{fold}_best.pt'\n","    if os.path.exists(checkpoint_path):\n","        _, _ = load_checkpoint(checkpoint_path, model)\n","        final_metrics = validate(model, val_loader, criterion, device, config)\n","    else:\n","        final_metrics = best_metrics\n","    \n","    # Generate final Grad-CAM visualizations\n","    print(\"\\nGenerating final Grad-CAM visualizations...\")\n","    try:\n","        val_batch = next(iter(val_loader))\n","        model.visualize_predictions_with_gradcam(\n","            val_batch, gradcam,\n","            f'{viz_dir}/final',\n","            num_samples=16  # More samples for final visualization\n","        )\n","        print(f\"✓ Final Grad-CAM visualizations saved\")\n","    except Exception as e:\n","        print(f\"⚠ Final Grad-CAM generation failed: {str(e)}\")\n","    \n","    # Cleanup hooks\n","    gradcam.remove_hooks()\n","    \n","    print(f\"\\n{'='*70}\")\n","    print(f\"FOLD {fold} RESULTS\")\n","    print(f\"{'='*70}\")\n","    print(f\"Training time:  {total_time/60:.2f} minutes\")\n","    print(f\"MAE:            {final_metrics['mae']:.4f}\")\n","    print(f\"MSE:            {final_metrics['mse']:.4f}\")\n","    print(f\"RMSE:           {final_metrics['rmse']:.4f}\")\n","    print(f\"R²:             {final_metrics['r2']:.4f}\")\n","    print(f\"{'='*70}\\n\")\n","    \n","    # Add final metrics to history\n","    history['final_metrics'] = final_metrics\n","    history['training_time'] = total_time\n","    \n","    return history\n","\n","\n","# ==================== VISUALIZATION ====================\n","def plot_training_history(all_histories, config):\n","    \"\"\"Plot training history for all folds\"\"\"\n","    os.makedirs(config.log_dir, exist_ok=True)\n","    \n","    fig = plt.figure(figsize=(20, 12))\n","    \n","    # Training loss\n","    ax1 = plt.subplot(3, 3, 1)\n","    for fold_idx, history in enumerate(all_histories):\n","        epochs = range(1, len(history['train_losses']) + 1)\n","        ax1.plot(epochs, history['train_losses'], marker='o', \n","                label=f'Fold {fold_idx+1}', linewidth=2, markersize=4, alpha=0.7)\n","    ax1.set_xlabel('Epoch', fontweight='bold')\n","    ax1.set_ylabel('Loss', fontweight='bold')\n","    ax1.set_title('Training Loss', fontweight='bold', fontsize=12)\n","    ax1.legend()\n","    ax1.grid(True, alpha=0.3)\n","    \n","    # Validation loss\n","    ax2 = plt.subplot(3, 3, 2)\n","    for fold_idx, history in enumerate(all_histories):\n","        val_epochs = np.arange(config.val_every_n_epochs, \n","                               len(history['val_losses']) * config.val_every_n_epochs + 1, \n","                               config.val_every_n_epochs)\n","        ax2.plot(val_epochs, history['val_losses'], marker='s', \n","                label=f'Fold {fold_idx+1}', linewidth=2, markersize=4, alpha=0.7)\n","    ax2.set_xlabel('Epoch', fontweight='bold')\n","    ax2.set_ylabel('Loss', fontweight='bold')\n","    ax2.set_title('Validation Loss', fontweight='bold', fontsize=12)\n","    ax2.legend()\n","    ax2.grid(True, alpha=0.3)\n","    \n","    # MAE\n","    ax3 = plt.subplot(3, 3, 3)\n","    for fold_idx, history in enumerate(all_histories):\n","        val_epochs = np.arange(config.val_every_n_epochs, \n","                               len(history['val_maes']) * config.val_every_n_epochs + 1, \n","                               config.val_every_n_epochs)\n","        ax3.plot(val_epochs, history['val_maes'], marker='^', \n","                label=f'Fold {fold_idx+1}', linewidth=2, markersize=4, alpha=0.7)\n","    ax3.set_xlabel('Epoch', fontweight='bold')\n","    ax3.set_ylabel('MAE', fontweight='bold')\n","    ax3.set_title('Validation MAE', fontweight='bold', fontsize=12)\n","    ax3.legend()\n","    ax3.grid(True, alpha=0.3)\n","    \n","    # RMSE\n","    ax4 = plt.subplot(3, 3, 4)\n","    for fold_idx, history in enumerate(all_histories):\n","        val_epochs = np.arange(config.val_every_n_epochs, \n","                               len(history['val_rmses']) * config.val_every_n_epochs + 1, \n","                               config.val_every_n_epochs)\n","        ax4.plot(val_epochs, history['val_rmses'], marker='d', \n","                label=f'Fold {fold_idx+1}', linewidth=2, markersize=4, alpha=0.7)\n","    ax4.set_xlabel('Epoch', fontweight='bold')\n","    ax4.set_ylabel('RMSE', fontweight='bold')\n","    ax4.set_title('Validation RMSE', fontweight='bold', fontsize=12)\n","    ax4.legend()\n","    ax4.grid(True, alpha=0.3)\n","    \n","    # R² Score\n","    ax5 = plt.subplot(3, 3, 5)\n","    for fold_idx, history in enumerate(all_histories):\n","        val_epochs = np.arange(config.val_every_n_epochs, \n","                               len(history['val_r2s']) * config.val_every_n_epochs + 1, \n","                               config.val_every_n_epochs)\n","        ax5.plot(val_epochs, history['val_r2s'], marker='*', \n","                label=f'Fold {fold_idx+1}', linewidth=2, markersize=6, alpha=0.7)\n","    ax5.set_xlabel('Epoch', fontweight='bold')\n","    ax5.set_ylabel('R²', fontweight='bold')\n","    ax5.set_title('Validation R²', fontweight='bold', fontsize=12)\n","    ax5.legend()\n","    ax5.grid(True, alpha=0.3)\n","    \n","    # Final metrics comparison\n","    fold_labels = [f'Fold {i+1}' for i in range(len(all_histories))]\n","    \n","    # Final MAE\n","    ax6 = plt.subplot(3, 3, 6)\n","    maes = [h['final_metrics']['mae'] for h in all_histories]\n","    bars = ax6.bar(fold_labels, maes, color='steelblue', edgecolor='black', alpha=0.7)\n","    ax6.axhline(np.mean(maes), color='red', linestyle='--', linewidth=2, \n","               label=f'Mean: {np.mean(maes):.4f}')\n","    for bar, val in zip(bars, maes):\n","        ax6.text(bar.get_x() + bar.get_width()/2., bar.get_height(), \n","                f'{val:.3f}', ha='center', va='bottom', fontweight='bold')\n","    ax6.set_ylabel('MAE', fontweight='bold')\n","    ax6.set_title('Final MAE by Fold', fontweight='bold', fontsize=12)\n","    ax6.legend()\n","    ax6.grid(True, alpha=0.3, axis='y')\n","    \n","    # Final RMSE\n","    ax7 = plt.subplot(3, 3, 7)\n","    rmses = [h['final_metrics']['rmse'] for h in all_histories]\n","    bars = ax7.bar(fold_labels, rmses, color='coral', edgecolor='black', alpha=0.7)\n","    ax7.axhline(np.mean(rmses), color='red', linestyle='--', linewidth=2, \n","               label=f'Mean: {np.mean(rmses):.4f}')\n","    for bar, val in zip(bars, rmses):\n","        ax7.text(bar.get_x() + bar.get_width()/2., bar.get_height(), \n","                f'{val:.3f}', ha='center', va='bottom', fontweight='bold')\n","    ax7.set_ylabel('RMSE', fontweight='bold')\n","    ax7.set_title('Final RMSE by Fold', fontweight='bold', fontsize=12)\n","    ax7.legend()\n","    ax7.grid(True, alpha=0.3, axis='y')\n","    \n","    # Final R²\n","    ax8 = plt.subplot(3, 3, 8)\n","    r2s = [h['final_metrics']['r2'] for h in all_histories]\n","    bars = ax8.bar(fold_labels, r2s, color='lightgreen', edgecolor='black', alpha=0.7)\n","    ax8.axhline(np.mean(r2s), color='red', linestyle='--', linewidth=2, \n","               label=f'Mean: {np.mean(r2s):.4f}')\n","    for bar, val in zip(bars, r2s):\n","        ax8.text(bar.get_x() + bar.get_width()/2., bar.get_height(), \n","                f'{val:.3f}', ha='center', va='bottom', fontweight='bold')\n","    ax8.set_ylabel('R²', fontweight='bold')\n","    ax8.set_title('Final R² by Fold', fontweight='bold', fontsize=12)\n","    ax8.legend()\n","    ax8.grid(True, alpha=0.3, axis='y')\n","    \n","    # Training time\n","    ax9 = plt.subplot(3, 3, 9)\n","    times = [h['training_time']/60 for h in all_histories]\n","    bars = ax9.bar(fold_labels, times, color='plum', edgecolor='black', alpha=0.7)\n","    for bar, val in zip(bars, times):\n","        ax9.text(bar.get_x() + bar.get_width()/2., bar.get_height(), \n","                f'{val:.1f}m', ha='center', va='bottom', fontweight='bold')\n","    ax9.set_ylabel('Time (minutes)', fontweight='bold')\n","    ax9.set_title('Training Time by Fold', fontweight='bold', fontsize=12)\n","    ax9.grid(True, alpha=0.3, axis='y')\n","    \n","    plt.tight_layout()\n","    plot_path = f'{config.log_dir}/training_results.png'\n","    plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n","    plt.close()\n","    \n","    print(f\"✓ Training plots saved to: {plot_path}\")\n","\n","\n","def save_results(all_histories, config):\n","    \"\"\"Save training results to JSON\"\"\"\n","    os.makedirs(config.log_dir, exist_ok=True)\n","    \n","    def convert_to_native(obj):\n","        \"\"\"Convert numpy types to native Python types\"\"\"\n","        if isinstance(obj, np.integer):\n","            return int(obj)\n","        elif isinstance(obj, np.floating):\n","            return float(obj)\n","        elif isinstance(obj, np.ndarray):\n","            return obj.tolist()\n","        elif isinstance(obj, dict):\n","            return {key: convert_to_native(value) for key, value in obj.items()}\n","        elif isinstance(obj, list):\n","            return [convert_to_native(item) for item in obj]\n","        else:\n","            return obj\n","    \n","    results = {\n","        'config': config.__dict__,\n","        'folds': []\n","    }\n","    \n","    for fold_idx, history in enumerate(all_histories):\n","        fold_results = {\n","            'fold': fold_idx + 1,\n","            'final_metrics': convert_to_native(history['final_metrics']),\n","            'training_time': float(history['training_time']),\n","            'num_epochs': len(history['train_losses'])\n","        }\n","        results['folds'].append(fold_results)\n","    \n","    # Compute aggregate statistics\n","    results['aggregate'] = {\n","        'mae': {\n","            'mean': float(np.mean([h['final_metrics']['mae'] for h in all_histories])),\n","            'std': float(np.std([h['final_metrics']['mae'] for h in all_histories])),\n","            'min': float(np.min([h['final_metrics']['mae'] for h in all_histories])),\n","            'max': float(np.max([h['final_metrics']['mae'] for h in all_histories]))\n","        },\n","        'rmse': {\n","            'mean': float(np.mean([h['final_metrics']['rmse'] for h in all_histories])),\n","            'std': float(np.std([h['final_metrics']['rmse'] for h in all_histories])),\n","            'min': float(np.min([h['final_metrics']['rmse'] for h in all_histories])),\n","            'max': float(np.max([h['final_metrics']['rmse'] for h in all_histories]))\n","        },\n","        'r2': {\n","            'mean': float(np.mean([h['final_metrics']['r2'] for h in all_histories])),\n","            'std': float(np.std([h['final_metrics']['r2'] for h in all_histories])),'min': float(np.min([h['final_metrics']['r2'] for h in all_histories])),\n","            'max': float(np.max([h['final_metrics']['r2'] for h in all_histories]))\n","        },\n","        'total_training_time': float(sum([h['training_time'] for h in all_histories]))\n","    }\n","    \n","    results_path = f'{config.log_dir}/results.json'\n","    with open(results_path, 'w') as f:\n","        json.dump(results, f, indent=4)\n","    \n","    print(f\"✓ Results saved to: {results_path}\")\n","\n","\n","# ==================== MAIN TRAINING PIPELINE ====================\n","def main():\n","    \"\"\"Main training pipeline\"\"\"\n","    print(\"\\n\" + \"=\"*70)\n","    print(\"  ViT FOR BMI PREDICTION - FIXED VERSION\")\n","    print(\"=\"*70 + \"\\n\")\n","    \n","    # Configuration\n","    config = TrainingConfig()\n","    \n","    # Set seed\n","    set_seed(config.seed)\n","    \n","    # Get device\n","    device = get_device()\n","    \n","    print(f\"\\nConfiguration:\")\n","    print(f\"  Folds:              {config.n_folds}\")\n","    print(f\"  Epochs:             {config.epochs}\")\n","    print(f\"  Batch size:         {config.batch_size}\")\n","    print(f\"  Learning rate:      {config.lr}\")\n","    print(f\"  ViT LR multiplier:  {config.vit_lr_multiplier}\")\n","    print(f\"  Loss function:      {config.loss_type}\")\n","    print(f\"  Mixed precision:    {config.use_amp}\")\n","    print(f\"  Adaptive pooling:   {config.use_adaptive_pooling}\")\n","    print(f\"  Output dir:         {config.output_dir}\")\n","    print(f\"  Log dir:            {config.log_dir}\\n\")\n","    \n","    # Train all folds\n","    all_histories = []\n","    \n","    for fold in range(1, config.n_folds + 1):\n","        try:\n","            history = train_fold(fold, config, device)\n","            all_histories.append(history)\n","        except Exception as e:\n","            print(f\"\\n✗ Error training fold {fold}: {str(e)}\")\n","            import traceback\n","            traceback.print_exc()\n","            continue\n","    \n","    if len(all_histories) == 0:\n","        print(\"\\n✗ No folds completed successfully!\")\n","        return\n","    \n","    # Plot results\n","    print(f\"\\n{'='*70}\")\n","    print(\"GENERATING PLOTS AND SAVING RESULTS\")\n","    print(f\"{'='*70}\\n\")\n","    \n","    try:\n","        plot_training_history(all_histories, config)\n","        save_results(all_histories, config)\n","    except Exception as e:\n","        print(f\"⚠ Error generating plots/results: {str(e)}\")\n","    \n","    # Print final summary\n","    print(f\"\\n{'='*70}\")\n","    print(\"FINAL CROSS-VALIDATION RESULTS\")\n","    print(f\"{'='*70}\")\n","    print(f\"{'Metric':<10} {'Mean':<12} {'Std':<12} {'Min':<12} {'Max':<12}\")\n","    print(\"-\"*70)\n","    \n","    maes = [h['final_metrics']['mae'] for h in all_histories]\n","    rmses = [h['final_metrics']['rmse'] for h in all_histories]\n","    r2s = [h['final_metrics']['r2'] for h in all_histories]\n","    \n","    print(f\"{'MAE':<10} {np.mean(maes):<12.4f} {np.std(maes):<12.4f} \"\n","          f\"{np.min(maes):<12.4f} {np.max(maes):<12.4f}\")\n","    print(f\"{'RMSE':<10} {np.mean(rmses):<12.4f} {np.std(rmses):<12.4f} \"\n","          f\"{np.min(rmses):<12.4f} {np.max(rmses):<12.4f}\")\n","    print(f\"{'R²':<10} {np.mean(r2s):<12.4f} {np.std(r2s):<12.4f} \"\n","          f\"{np.min(r2s):<12.4f} {np.max(r2s):<12.4f}\")\n","    \n","    total_time = sum([h['training_time'] for h in all_histories])\n","    print(f\"\\nTotal training time: {total_time/60:.2f} minutes ({total_time/3600:.2f} hours)\")\n","    print(f\"Completed folds: {len(all_histories)}/{config.n_folds}\")\n","    print(f\"{'='*70}\\n\")\n","    \n","    # Performance interpretation\n","    mean_mae = np.mean(maes)\n","    mean_r2 = np.mean(r2s)\n","    \n","    print(\"Performance Interpretation:\")\n","    print(\"-\" * 70)\n","    if mean_mae < 2.0:\n","        print(f\"✓ Excellent MAE: {mean_mae:.2f} BMI units\")\n","    elif mean_mae < 3.0:\n","        print(f\"✓ Good MAE: {mean_mae:.2f} BMI units\")\n","    elif mean_mae < 4.0:\n","        print(f\"○ Acceptable MAE: {mean_mae:.2f} BMI units\")\n","    else:\n","        print(f\"⚠ High MAE: {mean_mae:.2f} BMI units - consider model improvements\")\n","    \n","    if mean_r2 > 0.90:\n","        print(f\"✓ Excellent R²: {mean_r2:.4f}\")\n","    elif mean_r2 > 0.80:\n","        print(f\"✓ Good R²: {mean_r2:.4f}\")\n","    elif mean_r2 > 0.70:\n","        print(f\"○ Acceptable R²: {mean_r2:.4f}\")\n","    else:\n","        print(f\"⚠ Low R²: {mean_r2:.4f} - consider model improvements\")\n","    \n","    # Check for suspicious results\n","    if mean_mae < 0.5:\n","        print(f\"\\n⚠ WARNING: MAE too low ({mean_mae:.2f}) - possible data leakage!\")\n","    if mean_r2 > 0.98:\n","        print(f\"⚠ WARNING: R² too high ({mean_r2:.4f}) - possible data leakage!\")\n","    \n","    print(f\"{'='*70}\\n\")\n","    \n","    print(\"✓ Training complete!\")\n","    print(f\"✓ Models saved in: {config.output_dir}\")\n","    print(f\"✓ Logs saved in: {config.log_dir}\")\n","    print(f\"✓ Grad-CAM visualizations saved in: {config.output_dir}/gradcam_fold_*\")\n","\n","\n","# ==================== INFERENCE UTILITIES ====================\n","def load_trained_model(fold, config, device):\n","    \"\"\"Load a trained model from checkpoint\"\"\"\n","    # Load preprocessing config\n","    preprocess_config, feature_encoders = load_config_and_encoders(config.preprocessed_dir)\n","    \n","    # Get feature dimensions from fold data\n","    train_df = pd.read_csv(f'{config.preprocessed_dir}/fold_{fold}/train.csv')\n","    \n","    # Count features\n","    categorical_cols = [col for col in train_df.columns if col.endswith('_encoded')]\n","    numerical_cols = []\n","    for col in train_df.columns:\n","        if col in ['name', 'image_path', 'bmi', 'image_stem', 'has_image']:\n","            continue\n","        if train_df[col].dtype in [np.float32, np.float64, np.int32, np.int64]:\n","            if any(col.endswith(suffix) for suffix in [\n","                '_zscore', '_percentile', '_squared', '_deviation', \n","                '_above_mean', '_product', '_interaction'\n","            ]):\n","                numerical_cols.append(col)\n","            elif col in ['bsa', 'ponderal_index', 'height_m', 'weight_kg', \n","                       'age', 'age_decade', 'height', 'weight']:\n","                numerical_cols.append(col)\n","    \n","    num_categorical = len(categorical_cols)\n","    num_numerical = len(numerical_cols)\n","    categorical_vocab_sizes = get_categorical_vocab_sizes(feature_encoders)\n","    \n","    # Create model\n","    model = ViTForBMI(\n","        num_categorical_features=num_categorical,\n","        categorical_vocab_sizes=categorical_vocab_sizes,\n","        num_numerical_features=num_numerical,\n","        embed_dim=config.embed_dim,\n","        fusion_dim=config.fusion_dim,\n","        unfreeze_last_n_layers=config.unfreeze_last_n_layers,\n","        use_adaptive_pooling=config.use_adaptive_pooling,\n","        dropout=config.dropout\n","    ).to(device)\n","    \n","    # Load checkpoint\n","    checkpoint_path = f'{config.output_dir}/fold_{fold}_best.pt'\n","    if not os.path.exists(checkpoint_path):\n","        raise FileNotFoundError(f\"Checkpoint not found: {checkpoint_path}\")\n","    \n","    epoch, metrics = load_checkpoint(checkpoint_path, model)\n","    \n","    print(f\"Loaded model from fold {fold}, epoch {epoch}\")\n","    print(f\"Checkpoint metrics: MAE={metrics['mae']:.4f}, R²={metrics['r2']:.4f}\")\n","    \n","    return model, metrics\n","\n","\n","@torch.no_grad()\n","def predict_single_image(model, image_path, categorical_features, numerical_features, config, device):\n","    \"\"\"Predict BMI for a single image\"\"\"\n","    from torchvision import transforms\n","    \n","    # Load preprocessing config\n","    preprocess_config, _ = load_config_and_encoders(config.preprocessed_dir)\n","    \n","    # Create transform\n","    transform = transforms.Compose([\n","        transforms.Resize((224, 224)),\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean=preprocess_config['mean'], std=preprocess_config['std'])\n","    ])\n","    \n","    # Load and preprocess image\n","    try:\n","        with Image.open(image_path) as image:\n","            image = image.convert('RGB')\n","            image_tensor = transform(image).unsqueeze(0).to(device)\n","    except Exception as e:\n","        raise ValueError(f\"Failed to load image: {e}\")\n","    \n","    # Prepare features\n","    if categorical_features is not None:\n","        categorical_features = torch.tensor(categorical_features, dtype=torch.long).unsqueeze(0).to(device)\n","    \n","    if numerical_features is not None:\n","        numerical_features = torch.tensor(numerical_features, dtype=torch.float32).unsqueeze(0).to(device)\n","    \n","    # Predict\n","    model.eval()\n","    prediction = model(image_tensor, categorical_features, numerical_features)\n","    \n","    return prediction.item()\n","\n","\n","# ==================== TEST MODEL CREATION ====================\n","if __name__ == \"__main__\":\n","    # Quick test of model creation\n","    print(\"\\n\" + \"=\"*70)\n","    print(\"  TESTING MODEL CREATION\")\n","    print(\"=\"*70 + \"\\n\")\n","    \n","    # Test model\n","    test_model = ViTForBMI(\n","        num_categorical_features=4,\n","        categorical_vocab_sizes=[10, 5, 2, 8],\n","        num_numerical_features=15,\n","        embed_dim=32,\n","        fusion_dim=512,\n","        use_adaptive_pooling=False,\n","        dropout=0.2\n","    )\n","    \n","    count_parameters(test_model)\n","    \n","    # Test forward pass\n","    batch_size = 16\n","    pixel_values = torch.randn(batch_size, 3, 224, 224)\n","    categorical_features = torch.randint(0, 5, (batch_size, 4))\n","    numerical_features = torch.randn(batch_size, 15)\n","    \n","    output = test_model(pixel_values, categorical_features, numerical_features)\n","    \n","    print(f\"Input shapes:\")\n","    print(f\"  Images: {pixel_values.shape}\")\n","    print(f\"  Categorical: {categorical_features.shape}\")\n","    print(f\"  Numerical: {numerical_features.shape}\")\n","    print(f\"\\nOutput shape: {output.shape}\")\n","    print(f\"\\n✓ Model test passed!\\n\")\n","    \n","    main()"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":436801,"sourceId":829570,"sourceType":"datasetVersion"}],"dockerImageVersionId":31153,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"papermill":{"default_parameters":{},"duration":83.634817,"end_time":"2025-10-19T02:58:18.923871","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-10-19T02:56:55.289054","version":"2.6.0"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"1971425e75cb41f2a92a3a43d62d18fd":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_90c1d53437674e978e5203c3c5d77cb0","IPY_MODEL_9db17bfcf9714326be8b073230b3daf3","IPY_MODEL_3228165e03ab4f96a04115e18b2cd8d5"],"layout":"IPY_MODEL_2b8d7cc92b1343bb833594391395033b","tabbable":null,"tooltip":null}},"2434dae71cd54bc0b88efeb8ec819a2c":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"26c7f3020ddd4d2b94a793699839558f":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2b8d7cc92b1343bb833594391395033b":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3228165e03ab4f96a04115e18b2cd8d5":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_d82b042bed914e55beb89e4407893341","placeholder":"​","style":"IPY_MODEL_79993a137fd645c0af956fd966e212e6","tabbable":null,"tooltip":null,"value":" 346M/346M [00:01&lt;00:00, 387MB/s]"}},"36ab40509cbe4be2b45756a12e9acbf0":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_4a10a4de3f294b299ce76469d6aee97c","placeholder":"​","style":"IPY_MODEL_477a05af90fe414aa89bcf36df729b40","tabbable":null,"tooltip":null,"value":" 502/502 [00:00&lt;00:00, 58.9kB/s]"}},"3bfa4c05bb0d43009a72d93bfc772d44":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b880f41d43314d82bfd62f692bc6a137","IPY_MODEL_ecc6e74081c54920b39384a2348c4bb5","IPY_MODEL_57773a6b587f48dbba52621943bd2abd"],"layout":"IPY_MODEL_a928d135883e415180e97704ffd63b96","tabbable":null,"tooltip":null}},"3e212a7206ec4b53ab69b7d2071da374":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"417648e8cb2a4f669b141ac2cb70940c":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4344c0f4f62d47baa7500c9c9a5fd7c4":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"477a05af90fe414aa89bcf36df729b40":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"47aed3bea40945e1b6e73443a7eb2fad":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4a10a4de3f294b299ce76469d6aee97c":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"57773a6b587f48dbba52621943bd2abd":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_47aed3bea40945e1b6e73443a7eb2fad","placeholder":"​","style":"IPY_MODEL_7ec0722444bc4496883acd9a5ad355bc","tabbable":null,"tooltip":null,"value":" 1288/60715 [00:03&lt;02:48, 352.52it/s]"}},"5b8de1ae6ed54c9192b42451edbdab58":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6845e89043bd4abea054ebabdab756ee":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"69c220612eb14f44a89d6b4fb40b04c7":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"73db25136a9247adacad9fb52187ba1e":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"79993a137fd645c0af956fd966e212e6":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"7ec0722444bc4496883acd9a5ad355bc":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"805caf3560664bf1af30a747d3672f71":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_81f8853469794d7c8008bc458021c652","IPY_MODEL_bc42c143d3e84320b49a9301ab0a5e45","IPY_MODEL_36ab40509cbe4be2b45756a12e9acbf0"],"layout":"IPY_MODEL_6845e89043bd4abea054ebabdab756ee","tabbable":null,"tooltip":null}},"81f8853469794d7c8008bc458021c652":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_69c220612eb14f44a89d6b4fb40b04c7","placeholder":"​","style":"IPY_MODEL_fe33a224617e4a8e9872f125cae7cf58","tabbable":null,"tooltip":null,"value":"config.json: 100%"}},"90c1d53437674e978e5203c3c5d77cb0":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_26c7f3020ddd4d2b94a793699839558f","placeholder":"​","style":"IPY_MODEL_4344c0f4f62d47baa7500c9c9a5fd7c4","tabbable":null,"tooltip":null,"value":"model.safetensors: 100%"}},"9996ffd4dcee4ef4a836f61048bf7ab0":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9db17bfcf9714326be8b073230b3daf3":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"success","description":"","description_allow_html":false,"layout":"IPY_MODEL_3e212a7206ec4b53ab69b7d2071da374","max":345579424.0,"min":0.0,"orientation":"horizontal","style":"IPY_MODEL_417648e8cb2a4f669b141ac2cb70940c","tabbable":null,"tooltip":null,"value":345579424.0}},"a928d135883e415180e97704ffd63b96":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b880f41d43314d82bfd62f692bc6a137":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_9996ffd4dcee4ef4a836f61048bf7ab0","placeholder":"​","style":"IPY_MODEL_2434dae71cd54bc0b88efeb8ec819a2c","tabbable":null,"tooltip":null,"value":"Quick validation:   2%"}},"bc42c143d3e84320b49a9301ab0a5e45":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"success","description":"","description_allow_html":false,"layout":"IPY_MODEL_73db25136a9247adacad9fb52187ba1e","max":502.0,"min":0.0,"orientation":"horizontal","style":"IPY_MODEL_5b8de1ae6ed54c9192b42451edbdab58","tabbable":null,"tooltip":null,"value":502.0}},"d82b042bed914e55beb89e4407893341":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ec2bfea5c8024adb8f9bd2a5c5658b2a":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ecc6e74081c54920b39384a2348c4bb5":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_allow_html":false,"layout":"IPY_MODEL_f98513c1631249e4af7b72bec2b714c4","max":60715.0,"min":0.0,"orientation":"horizontal","style":"IPY_MODEL_ec2bfea5c8024adb8f9bd2a5c5658b2a","tabbable":null,"tooltip":null,"value":1288.0}},"f98513c1631249e4af7b72bec2b714c4":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fe33a224617e4a8e9872f125cae7cf58":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}}},"version_major":2,"version_minor":0}}},"nbformat":4,"nbformat_minor":5}